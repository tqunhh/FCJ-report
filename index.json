[{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.7-security/5.7.1-encryption-at-rest/","title":"Encryption at Rest","tags":[],"description":"","content":"Encryption at Rest - Data Protection What is Encryption at Rest? Encryption at Rest means encrypting data when it\u0026rsquo;s stored on disk (at rest), as opposed to data in transit (being transmitted over network).\nWhy it matters:\nProtects data if physical storage is compromised Compliance requirement (GDPR, HIPAA, PCI-DSS) Defense in depth security strategy Minimal performance impact with AWS managed encryption Implementation Overview We implemented encryption for:\nDynamoDB Tables (6 tables) - Using AWS KMS S3 Buckets (2 buckets) - Using AES-256 DynamoDB Encryption Tables Encrypted All DynamoDB tables now use AWS KMS encryption:\nArticlesTable UserFavoritesTable GalleryPhotosTable GalleryTrendsTable UserProfilesTable LocationCacheTable Configuration CloudFormation Template Changes:\nArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#34;articles-${Environment}\u0026#34; # ... other properties ... # ‚úÖ ADDED: KMS Encryption SSESpecification: SSEEnabled: true SSEType: KMS How it Works 1. Application writes data to DynamoDB ‚Üì 2. DynamoDB encrypts data using KMS key ‚Üì 3. Encrypted data stored on disk ‚Üì 4. When reading, DynamoDB decrypts automatically ‚Üì 5. Application receives decrypted data Key Points:\nEncryption/decryption is automatic No code changes required Transparent to application Uses AWS-managed KMS keys Benefits ‚úÖ Security:\nData encrypted at rest Protection against disk theft Compliance with regulations ‚úÖ Ease of Use:\nFully managed by AWS Automatic key rotation No key management burden ‚úÖ Performance:\nMinimal latency impact (\u0026lt;1ms) No application changes needed Cost DynamoDB KMS Encryption:\nKMS key: $1/month API calls: ~$0.03 per 10,000 requests Estimated: $5-10/month for typical usage S3 Encryption Buckets Encrypted Both S3 buckets now use AES-256 encryption:\nArticleImagesBucket (user uploads) StaticSiteBucket (frontend assets) Configuration CloudFormation Template Changes:\nArticleImagesBucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#34;travel-guide-images-${Environment}-${AWS::AccountId}\u0026#34; # ... other properties ... # ‚úÖ ADDED: AES-256 Encryption BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 BucketKeyEnabled: true How it Works 1. Application uploads file to S3 ‚Üì 2. S3 encrypts file using AES-256 ‚Üì 3. Encrypted file stored on disk ‚Üì 4. When downloading, S3 decrypts automatically ‚Üì 5. Application receives decrypted file Key Points:\nEncryption is server-side Automatic for all objects No client-side changes needed Uses S3-managed keys (SSE-S3) Benefits ‚úÖ Security:\nAll files encrypted Protection against unauthorized access Compliance ready ‚úÖ Cost:\nFREE - No additional cost for SSE-S3 No performance impact ‚úÖ Simplicity:\nEnabled at bucket level Applies to all objects No code changes Deployment Step 1: Update CloudFormation Template File: travel-guide-backend/core-infra/template.yaml\nChanges made:\nAdded SSESpecification to all DynamoDB tables Added BucketEncryption to all S3 buckets Step 2: Deploy Stack cd travel-guide-backend # Deploy core infrastructure ./scripts/deploy-core.sh staging # Or manually: sam build -t core-infra/template.yaml sam deploy \\ --stack-name travel-guide-core-staging \\ --parameter-overrides Environment=staging \\ --capabilities CAPABILITY_IAM Step 3: Verify Encryption Verify DynamoDB:\n# Check table encryption aws dynamodb describe-table \\ --table-name articles-staging \\ --query \u0026#39;Table.SSEDescription\u0026#39; # Expected output: { \u0026#34;Status\u0026#34;: \u0026#34;ENABLED\u0026#34;, \u0026#34;SSEType\u0026#34;: \u0026#34;KMS\u0026#34; } Verify S3:\n# Check bucket encryption aws s3api get-bucket-encryption \\ --bucket travel-guide-images-staging-123456789012 # Expected output: { \u0026#34;ServerSideEncryptionConfiguration\u0026#34;: { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ApplyServerSideEncryptionByDefault\u0026#34;: { \u0026#34;SSEAlgorithm\u0026#34;: \u0026#34;AES256\u0026#34; }, \u0026#34;BucketKeyEnabled\u0026#34;: true } ] } } Testing Test DynamoDB Encryption # Create test article curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test Encryption\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Sensitive data\u0026#34;, \u0026#34;latitude\u0026#34;: 10.8231, \u0026#34;longitude\u0026#34;: 106.6297 }\u0026#39; # Verify data is encrypted in DynamoDB # (You cannot see encrypted data directly - it\u0026#39;s transparent) # Read article back curl https://api.example.com/articles/{article-id} \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; # Data should be readable (decrypted automatically) Test S3 Encryption # Upload test image aws s3 cp test-image.jpg \\ s3://travel-guide-images-staging-123456789012/test/ # Check object encryption aws s3api head-object \\ --bucket travel-guide-images-staging-123456789012 \\ --key test/test-image.jpg \\ --query \u0026#39;ServerSideEncryption\u0026#39; # Expected: \u0026#34;AES256\u0026#34; Compliance Regulations Supported ‚úÖ GDPR (EU)\nArticle 32: Security of processing Encryption at rest required ‚úÖ HIPAA (Healthcare)\nTechnical safeguards Encryption required for PHI ‚úÖ PCI-DSS (Payment)\nRequirement 3.4: Encryption of cardholder data Encryption at rest mandatory ‚úÖ SOC 2\nSecurity principle Encryption controls Monitoring CloudWatch Metrics DynamoDB:\nMonitor KMS API calls Track encryption errors Alert on decryption failures S3:\nMonitor encryption status Track unencrypted uploads (should be 0) CloudTrail Logging Enable CloudTrail to audit:\nKMS key usage S3 encryption changes Access to encrypted data # Check CloudTrail logs aws cloudtrail lookup-events \\ --lookup-attributes AttributeKey=ResourceType,AttributeValue=AWS::KMS::Key \\ --max-results 10 Best Practices 1. Use AWS-Managed Keys ‚úÖ Recommended: AWS-managed KMS keys\nAutomatic rotation No key management Lower cost ‚ùå Avoid: Customer-managed keys (unless required)\nManual rotation Key management burden Higher cost 2. Enable Encryption by Default ‚úÖ Do: Enable at bucket/table level\nApplies to all data Cannot be bypassed ‚ùå Don\u0026rsquo;t: Rely on object-level encryption\nEasy to forget Inconsistent protection 3. Monitor Encryption Status ‚úÖ Do: Set up CloudWatch alarms\nAlert on encryption disabled Track unencrypted objects 4. Document Encryption Keys ‚úÖ Do: Document which keys encrypt what\nKMS key IDs Key policies Rotation schedules Troubleshooting Issue 1: KMS Access Denied Error:\nAccessDeniedException: User is not authorized to perform: kms:Decrypt Solution:\nAdd KMS permissions to Lambda execution role Grant kms:Decrypt and kms:DescribeKey { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:*:*:key/*\u0026#34; } Issue 2: S3 Encryption Not Applied Problem: Old objects not encrypted\nSolution:\nEncryption only applies to new objects Re-upload existing objects: # Copy object to itself (re-encrypts) aws s3 cp \\ s3://bucket/key \\ s3://bucket/key \\ --metadata-directive REPLACE Issue 3: Performance Impact Problem: Slight latency increase\nSolution:\nNormal with encryption (\u0026lt;1ms) Use S3 Bucket Keys to reduce KMS calls Already enabled: BucketKeyEnabled: true Cost Optimization DynamoDB KMS Reduce costs:\nUse AWS-managed keys (cheaper) Enable S3 Bucket Keys (reduces KMS API calls) Monitor KMS usage with Cost Explorer S3 AES-256 No cost:\nSSE-S3 (AES-256) is FREE No additional charges No performance impact Key Takeaways Encryption at rest protects data on disk DynamoDB uses KMS - automatic encryption/decryption S3 uses AES-256 - free and transparent No code changes required - fully managed Compliance ready - meets GDPR, HIPAA, PCI-DSS Minimal cost - $5-10/month for DynamoDB KMS "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.6-cloudfront-s3-location/5.6.1-s3-presigned-upload/","title":"S3 Pre-signed URL Upload","tags":[],"description":"","content":"Amazon S3 Upload Web (Pre-signed URL) Why Use Pre-signed URLs? Instead of uploading images through the backend (consuming bandwidth/risking timeout), we use a two-step approach:\nFrontend requests a temporary upload URL from backend Frontend uploads directly to S3 Benefits:\nFast, reduces backend load Time-limited URL (security) No AWS credentials needed on client Direct upload to S3 S3 Bucket Configuration ArticleImagesBucket Setup Key configurations:\nVersioning: Enabled CORS: Allows uploads from frontend Access: Private (accessed via CloudFront OAI) Event notifications: Triggers SQS for image processing CORS Configuration [ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;] } ] Note: In production, replace \u0026quot;*\u0026quot; with specific domains.\nAPI: Generate Pre-signed URL Lambda Function Function: GetUploadUrlFunction\nAPI Path: POST /upload-url\nAuthentication: Cognito JWT required\nRequest Flow Frontend calls POST {API_BASE}/upload-url with JWT token Backend generates pre-signed URL Backend returns: uploadUrl: Temporary S3 upload URL key: Object path in S3 expiresIn: 900 seconds (15 minutes) Response Example { \u0026#34;uploadUrl\u0026#34;: \u0026#34;https://s3.ap-southeast-1.amazonaws.com/...\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;articles/\u0026lt;articleId\u0026gt;/raw/\u0026lt;uuid\u0026gt;.jpg\u0026#34;, \u0026#34;articleId\u0026#34;: \u0026#34;\u0026lt;uuid\u0026gt;\u0026#34;, \u0026#34;expiresIn\u0026#34;: 900 } Lambda Code Structure import boto3 import uuid from datetime import datetime s3_client = boto3.client(\u0026#39;s3\u0026#39;) BUCKET_NAME = os.environ[\u0026#39;ARTICLE_IMAGES_BUCKET\u0026#39;] def lambda_handler(event, context): # Get user info from Cognito user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] # Generate unique key article_id = str(uuid.uuid4()) file_key = f\u0026#34;articles/{article_id}/raw/{uuid.uuid4()}.jpg\u0026#34; # Generate pre-signed URL upload_url = s3_client.generate_presigned_url( \u0026#39;put_object\u0026#39;, Params={ \u0026#39;Bucket\u0026#39;: BUCKET_NAME, \u0026#39;Key\u0026#39;: file_key, \u0026#39;ContentType\u0026#39;: \u0026#39;image/jpeg\u0026#39; }, ExpiresIn=900 # 15 minutes ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;uploadUrl\u0026#39;: upload_url, \u0026#39;key\u0026#39;: file_key, \u0026#39;articleId\u0026#39;: article_id, \u0026#39;expiresIn\u0026#39;: 900 }) } Frontend Upload Implementation React Example // Step 1: Get pre-signed URL const getUploadUrl = async () =\u0026gt; { const response = await fetch(`${API_BASE}/upload-url`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${accessToken}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); return await response.json(); }; // Step 2: Upload file directly to S3 const uploadImage = async (file) =\u0026gt; { // Get upload URL const { uploadUrl, key, articleId } = await getUploadUrl(); // Upload to S3 const uploadResponse = await fetch(uploadUrl, { method: \u0026#39;PUT\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: file.type }, body: file }); if (uploadResponse.ok) { console.log(\u0026#39;Upload successful!\u0026#39;); return { key, articleId }; } else { throw new Error(\u0026#39;Upload failed\u0026#39;); } }; // Usage const handleFileSelect = async (event) =\u0026gt; { const file = event.target.files[0]; try { const result = await uploadImage(file); console.log(\u0026#39;Image uploaded:\u0026#39;, result); } catch (error) { console.error(\u0026#39;Upload error:\u0026#39;, error); } }; IAM Permissions Lambda Execution Role The Lambda function needs s3:PutObject permission:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::article-images-bucket/*\u0026#34; } ] } Note: Frontend doesn\u0026rsquo;t need AWS credentials because it uses pre-signed URLs.\nOperations \u0026amp; Troubleshooting Common Issues 1. 403 AccessDenied on PUT Causes:\nURL expired (\u0026gt; 15 minutes) Content-Type mismatch (if enforced in pre-signed URL) Bucket policy blocking access Solution:\nRequest new upload URL Ensure Content-Type matches Check bucket policy 2. CORS Error Causes:\nCORS not configured on bucket Wrong method/headers in request Solution:\nVerify CORS configuration on ArticleImagesBucket Ensure frontend sends correct headers Check browser console for specific CORS error 3. Object Uploaded but Can\u0026rsquo;t Load Cause:\nBucket is private Solution:\nLoad images via CloudFront (OAI) Or generate pre-signed download URLs Monitoring CloudWatch Metrics to track:\nNumber of pre-signed URL requests Upload success/failure rate Average upload time S3 PUT request count Security Best Practices Time-limited URLs: Keep expiration short (15 minutes) Content-Type validation: Enforce image types only File size limits: Set max file size in API User authentication: Always require JWT token Bucket encryption: Enable S3 encryption at rest Logging: Enable S3 access logs for audit Cost Optimization Strategies:\nUse S3 Intelligent-Tiering for infrequently accessed images Enable S3 Transfer Acceleration for global users (optional) Set lifecycle policies to delete incomplete multipart uploads Monitor S3 storage costs with Cost Explorer Key Takeaways Pre-signed URLs enable secure, direct uploads without credentials CORS configuration is essential for browser uploads Time-limited URLs enhance security CloudFront should be used for serving images (not direct S3) IAM least privilege - Lambda only needs PutObject permission "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.3-backend-articles/5.3.1-lambda-service-backend/","title":" Backend ‚Äì Article Service ","tags":[],"description":"","content":"Template Code Structure Lambda Functions Overview API Gateway Integration Summary of Lambda Functions This section provides a concise overview of all Lambda functions used in the Article, Profile, and Favorites modules. These functions collectively form the backend workflow of the content system.\n1. Article Functions (CRUD + Search + Upload) CreateArticleFunction\nEndpoint: POST /articles (Auth required) Role: Creates a new article, processes image metadata, resolves geolocation using AWS Location, saves data into DynamoDB and S3. GetArticleFunction\nEndpoint: GET /articles/{articleId} (Public) Role: Retrieves a single article and images while applying visibility rules. ListArticlesFunction\nEndpoint: GET /articles (Auth required) Role: Lists public articles enriched with profile data and image metadata. UpdateArticleFunction\nEndpoint: PATCH /articles/{articleId} Role: Verifies ownership and updates article content, visibility, location, and images. DeleteArticleFunction\nEndpoint: DELETE /articles/{articleId} Role: Permanently deletes the article and related images from S3. SearchArticlesFunction\nEndpoint: GET /search (Public) Role: Performs keyword/tag search using DynamoDB queries. GetUploadUrlFunction\nEndpoint: POST /upload-url (Auth required) Role: Generates a presigned URL for uploading images directly to S3. 2. User Profile Function GetUserArticlesFunction Endpoint: GET /users/{userId}/articles Role: Retrieves all public articles created by a specific user for their profile page. 3. Favorites Functions FavoriteArticleFunction\nEndpoint: POST /articles/{articleId}/favorite Role: Adds (userId, articleId) mapping into UserFavoritesTable. UnfavoriteArticleFunction\nEndpoint: DELETE /articles/{articleId}/favorite Role: Removes a favorite record. ListFavoriteArticlesFunction\nEndpoint: GET /me/favorites Role: Returns all articles favorited by the authenticated user. Overall Summary These Lambda functions integrate DynamoDB, S3, Cognito, and AWS Location to create a modular, scalable backend powering the article management system.\n"},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.5-auth-cognito-iam/5.5.1-cognito-setup/","title":"AWS Cognito Setup","tags":[],"description":"","content":"AWS Cognito - User Authentication Service Introduction to AWS Cognito Amazon Cognito is an Identity Management service built to help your application handle user registration, login, and authentication without designing a complex authentication system from scratch. Cognito supports horizontal scaling, high availability, and security according to AWS Security standards.\nThrough Cognito User Pool, applications can authenticate users using email, phone number, password, or social login providers (Google, Facebook, Apple). Additionally, Cognito issues JWT tokens so Frontend and Backend applications can control access without storing session state on the server.\nUser Pool is responsible for:\nManaging user identities OTP authentication Storing user profiles Generating tokens App Client and Hosted UI provide integrated login interface, serving Web and Mobile applications well without building login screens from scratch.\nWorkshop Overview In this workshop, you will deploy a complete authentication system based on Cognito, simulating all components commonly found in enterprise environments.\nWe will use two application environment groups:\n\u0026ldquo;Frontend App\u0026rdquo; This is a React application simulating the user interface (Client Application). This application will integrate Cognito\u0026rsquo;s Hosted UI and manage id_token, access_token, and refresh_token.\nFrontend will be configured to:\nConnect to Cognito Domain Receive tokens after user login Call protected APIs using Access Token \u0026ldquo;Backend API\u0026rdquo; This simulates the enterprise Service Layer. Backend includes API Gateway, Cognito Authorizer, and Lambda functions for processing logic.\nAPIs in this environment will:\nRequire valid access_token for access Validate tokens using Cognito before forwarding to Lambda Read user information from JWT Simulate authentication \u0026amp; authorization behavior of production backend systems Step 1: Create User Pool Navigate to AWS Console ‚Üí Cognito ‚Üí User Pools ‚Üí Create user pool\nConfiguration:\nPool name: TravelGuideUserPool-staging Sign-in options: Email Attributes: email, name Step 2: Password Policy Configure password requirements:\nAt least 1 number At least 1 lowercase letter At least 1 uppercase letter Why these requirements?\nEnsures strong passwords Meets security compliance standards Protects against brute force attacks Step 3: Create App Client Navigate to: App integration ‚Üí App clients ‚Üí Create app client\nSteps:\nEnter app client name Select Authentication flows: ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH Create app client Authentication Flows Explained:\nUSER_PASSWORD_AUTH: Direct username/password authentication REFRESH_TOKEN_AUTH: Token refresh capability USER_SRP_AUTH: Secure Remote Password protocol (recommended) Step 4: Create Test User Navigate to: User Pool ‚Üí Users ‚Üí Create user\nUser Information:\nUsername Email address Phone number (optional) Temporary password Note: User will need to change password on first login.\nStep 5: Sign Up Flow Cognito automatically handles OTP verification - no backend code needed!\nSign Up Process:\nUser enters email and password Cognito sends verification code to email User enters code to verify Account is activated Benefits:\n‚úÖ No custom email service needed ‚úÖ Built-in rate limiting ‚úÖ Automatic retry logic ‚úÖ Secure code generation Token Management JWT Tokens Explained Cognito issues three types of tokens:\n1. ID Token Contains user identity information Used by frontend to display user data Short-lived (1 hour default) { \u0026#34;sub\u0026#34;: \u0026#34;user-uuid\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;cognito:username\u0026#34;: \u0026#34;johndoe\u0026#34; } 2. Access Token Used to authorize API requests Sent in Authorization header Validated by API Gateway Authorization: Bearer \u0026lt;access_token\u0026gt; 3. Refresh Token Used to obtain new ID and Access tokens Long-lived (30 days default) Stored securely on client Token Flow 1. User Login ‚Üì 2. Cognito validates credentials ‚Üì 3. Cognito issues tokens ‚Üì 4. Frontend stores tokens ‚Üì 5. Frontend calls API with Access Token ‚Üì 6. API Gateway validates token ‚Üì 7. Lambda processes request Frontend Integration React Example import { CognitoUserPool, CognitoUser, AuthenticationDetails } from \u0026#39;amazon-cognito-identity-js\u0026#39;; const poolData = { UserPoolId: \u0026#39;us-east-1_XXXXXXXXX\u0026#39;, ClientId: \u0026#39;your-app-client-id\u0026#39; }; const userPool = new CognitoUserPool(poolData); // Sign In function signIn(username, password) { const authenticationData = { Username: username, Password: password, }; const authenticationDetails = new AuthenticationDetails(authenticationData); const userData = { Username: username, Pool: userPool }; const cognitoUser = new CognitoUser(userData); cognitoUser.authenticateUser(authenticationDetails, { onSuccess: (result) =\u0026gt; { const accessToken = result.getAccessToken().getJwtToken(); const idToken = result.getIdToken().getJwtToken(); const refreshToken = result.getRefreshToken().getToken(); // Store tokens localStorage.setItem(\u0026#39;accessToken\u0026#39;, accessToken); localStorage.setItem(\u0026#39;idToken\u0026#39;, idToken); localStorage.setItem(\u0026#39;refreshToken\u0026#39;, refreshToken); }, onFailure: (err) =\u0026gt; { console.error(err); } }); } Backend Integration API Gateway Cognito Authorizer Configuration:\nCreate Cognito User Pool Authorizer Specify User Pool ID Token source: Authorization header Attach to API routes Lambda receives user info:\ndef lambda_handler(event, context): # User info from Cognito claims = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;] user_id = claims[\u0026#39;sub\u0026#39;] email = claims[\u0026#39;email\u0026#39;] username = claims[\u0026#39;cognito:username\u0026#39;] return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Hello {username}!\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } Key Takeaways Cognito User Pool manages user identities JWT Tokens provide stateless authentication Hosted UI simplifies frontend integration Cognito Authorizer secures API Gateway No session management needed on backend Scalable and highly available by default Best Practices Use SRP authentication for enhanced security Enable MFA for sensitive operations Rotate refresh tokens regularly Store tokens securely (not in localStorage for production) Implement token refresh logic Use HTTPS for all communications Set appropriate token expiration times "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.1-iac-strategy/","title":"IaC Strategy &amp; Tool Selection","tags":[],"description":"","content":"Infrastructure as Code Strategy Why Infrastructure as Code? Infrastructure as Code (IaC) allows us to:\nVersion Control: Track infrastructure changes in Git Reproducibility: Deploy identical environments consistently Automation: Reduce manual errors and deployment time Documentation: Code serves as living documentation Collaboration: Team members can review and contribute Why CloudFormation/SAM? For the Travel Guide Application, we chose AWS CloudFormation with SAM (Serverless Application Model) for the following reasons:\nAdvantages 1. Native AWS Integration\nFirst-class support for all AWS services No additional state management required Automatic rollback on failures Built-in drift detection 2. SAM Simplification\nSimplified syntax for Lambda, API Gateway, DynamoDB Local testing capabilities (sam local) Built-in best practices for serverless Automatic IAM role generation 3. No Additional Cost\nCloudFormation is free (pay only for resources) No need for external state storage No licensing fees 4. AWS Ecosystem\nIntegrates with CodePipeline, CodeBuild CloudWatch integration for monitoring AWS Console visualization Trade-offs vs Alternatives Feature CloudFormation/SAM Terraform AWS CDK Pulumi Learning Curve Medium Medium High High Syntax YAML/JSON HCL TypeScript/Python TypeScript/Python/Go State Management AWS-managed Manual/Cloud AWS-managed Cloud Multi-Cloud ‚ùå No ‚úÖ Yes ‚ùå No ‚úÖ Yes Cost Free Free (Cloud paid) Free Free (Cloud paid) AWS Integration Native Good Native Good Serverless Support Excellent (SAM) Good Excellent Good Template Organization Strategy Our templates are organized as follows:\ninfrastructure/ ‚îú‚îÄ‚îÄ core/ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Core stack (DynamoDB, S3, Cognito) ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îú‚îÄ‚îÄ auth/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Auth service Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ articles/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Article service Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ media/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Media processing Lambda functions ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ parameters/ ‚îÇ ‚îú‚îÄ‚îÄ staging.json # Staging environment parameters ‚îÇ ‚îî‚îÄ‚îÄ prod.json # Production environment parameters ‚îî‚îÄ‚îÄ scripts/ ‚îú‚îÄ‚îÄ deploy.sh # Main deployment orchestration ‚îú‚îÄ‚îÄ deploy-core.sh # Core stack deployment ‚îî‚îÄ‚îÄ deploy-service.sh # Service stack deployment Basic CloudFormation Template Structure AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Core Infrastructure Parameters: Environment: Type: String Default: staging AllowedValues: [staging, prod] Resources: # DynamoDB Tables ArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#39;${AWS::StackName}-articles\u0026#39; BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: articleId AttributeType: S KeySchema: - AttributeName: articleId KeyType: HASH Outputs: ArticlesTableName: Description: Articles DynamoDB Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; SAM Transform Example SAM simplifies Lambda and API Gateway definitions:\n# Without SAM (CloudFormation only) CreateArticleFunction: Type: AWS::Lambda::Function Properties: FunctionName: create-article Runtime: python3.11 Handler: index.handler Code: S3Bucket: my-bucket S3Key: function.zip Role: !GetAtt LambdaRole.Arn # With SAM CreateArticleFunction: Type: AWS::Serverless::Function Properties: CodeUri: ./src Handler: create_article.handler Runtime: python3.11 Events: CreateArticle: Type: Api Properties: Path: /articles Method: post Key Takeaways CloudFormation/SAM is ideal for AWS-only deployments SAM significantly reduces boilerplate for serverless applications Native integration eliminates state management complexity Template organization is crucial for maintainability Trade-offs exist - multi-cloud requires different tools When to Consider Alternatives Terraform: If you need multi-cloud support or team is already familiar AWS CDK: If you prefer programming languages over YAML Pulumi: If you want full programming language power with multi-cloud "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/5.4.1-content-moderation/","title":"Lambda Content Moderation","tags":[],"description":"","content":"Purpose Automatically moderate image content to detect violations such as:\nExplicit/suggestive content Violence Drugs/alcohol Offensive symbols Main Code Explanation a. Receive message from SQS Extract bucket and key information from S3 event:\nfor sqs_record in event.get(\u0026#39;Records\u0026#39;, []): try: # Parse S3 event from SQS body s3_event = json.loads(sqs_record[\u0026#39;body\u0026#39;]) # Process each S3 record for s3_record in s3_event.get(\u0026#39;Records\u0026#39;, []): try: bucket = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] b. Call Rekognition detect_moderation_labels AWS Rekognition analyzes the image and returns violation labels:\nmoderation_result = moderate_image(bucket, key) if \u0026#39;error\u0026#39; in moderation_result: print(f\u0026#34;Moderation failed: {moderation_result[\u0026#39;error\u0026#39;]}\u0026#34;) results[\u0026#39;errors\u0026#39;] += 1 continue results[\u0026#39;processed\u0026#39;] += 1 if moderation_result[\u0026#39;passed\u0026#39;]: results[\u0026#39;approved\u0026#39;] += 1 mark_article_as_approved(article_id) # Only forward to next queue if approved final_status = { \u0026#39;moderationStatus\u0026#39;: \u0026#39;approved\u0026#39;, \u0026#39;processed\u0026#39;: True } forward_to_next_queue(bucket, key, article_id, final_status) else: results[\u0026#39;rejected\u0026#39;] += 1 action_result, owner_id = handle_moderation_failure( bucket, key, article_id, moderation_result ) if action_result in results[\u0026#39;actions\u0026#39;]: results[\u0026#39;actions\u0026#39;][action_result] += 1 c. Handle violations - Publish SNS to trigger SES email If content violates policies, send admin notification:\n# Always send admin notification for deleted/quarantined content if action_result in [\u0026#39;deleted\u0026#39;, \u0026#39;quarantined\u0026#39;]: print(f\u0026#34;üìß Sending admin notification for {action_result} content\u0026#34;) send_admin_notification(article_id, key, moderation_result, owner_id) elif moderation_result[\u0026#39;maxSeverity\u0026#39;] in [\u0026#39;critical\u0026#39;, \u0026#39;high\u0026#39;]: print(f\u0026#34;üìß Sending admin notification for {moderation_result[\u0026#39;maxSeverity\u0026#39;]} severity\u0026#34;) send_admin_notification(article_id, key, moderation_result, owner_id) # DO NOT forward to next queue if rejected/deleted # Pipeline stops here, user already received deletion email print(f\u0026#34;‚ö†Ô∏è Pipeline stopped for rejected image: {key}\u0026#34;) print(f\u0026#34; User notification already sent via send_user_deletion_email()\u0026#34;) Demo: Email Notification for Violent Content When violent content is detected, the system automatically sends email notifications:\nProcessing Flow Image uploaded to S3 S3 event ‚Üí SQS Queue Lambda reads from SQS Rekognition analyzes content If passed ‚Üí Forward to Detect Labels queue If failed ‚Üí Send notification + Stop pipeline Result Approved images continue to label detection Violated images are quarantined/deleted Admin and user receive email notifications "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://tqunhh.github.io/FCJ-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Takeaways from \u0026ldquo;AI/ML/GenAI on AWS Workshop\u0026rdquo; Event Purpose Provide an overview of the AI/ML ecosystem on AWS Introduce Amazon SageMaker ‚Äì end-to-end ML platform Explore Generative AI with Amazon Bedrock Hands-on building GenAI Chatbot Get familiar with Prompt Engineering, RAG, Bedrock Agents \u0026amp; Guardrails Connect with the AI/ML community in Vietnam Speaker List Lam Tuan Kiet - Senior DevOps Engineer, FPT Software Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Highlighted Content AWS AI/ML Services Overview Attendees were introduced to the overall picture of AI/ML services on AWS, from data preparation, training, deployment to monitoring and automation.\nAmazon SageMaker ‚Äì End-to-End ML Platform Data preparation \u0026amp; labeling Model training, hyperparameter tuning Deployment strategies and best practices MLOps integration (CI/CD, pipeline, monitoring) Live Demo: SageMaker Studio walkthrough helps visualize ML workflow from A ‚Üí Z in a real environment.\nGenerative AI with Amazon Bedrock Foundation Models\nComparison and guidance on choosing between Claude, Llama, Titan Clearly understand strengths and weaknesses according to each use case RAG (Retrieval-Augmented Generation)\nOverall architecture of RAG system How to integrate Knowledge Base with LLM RAG use cases in enterprises (internal search, support chatbot, QA system) Bedrock Agents \u0026amp; Guardrails Build multi-step workflows Integrate tools, APIs Set up safety \u0026amp; content filtering to ensure safe and compliant AI Live Demo: Building Generative AI Chatbot using Amazon Bedrock + RAG ‚Äì an extremely impressive highlight of the workshop.\nWhat I Learned After the workshop, I gained:\nAmazon SageMaker Clearly understand end-to-end ML on AWS Grasp the training, tuning \u0026amp; deployment process Visualize MLOps workflow and automation methods Generative AI with Amazon Bedrock How to choose appropriate Foundation Model Apply Prompt Engineering for LLM Understand the essence of Chain-of-Thought \u0026amp; Few-shot learning RAG How LLM combines with internal data RAG applications in enterprise AI systems Bedrock Agents \u0026amp; Guardrails Multi-step orchestration with AI Agents Ensure safety, control generated content Hands-on Experience Practice with SageMaker Studio Build real GenAI chatbot Event Experience Participating in the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was a valuable experience, helping me expand my perspective on how AWS is supporting the building and operation of AI/ML systems, especially Generative AI, in modern cloud environments. The event was not only technical but also clearly conveyed the mindset of deploying AI into real-world problems.\nLearning from highly specialized speakers The speakers all came from teams with experience deploying AI/ML in production environments. Instead of just talking about theory, they focused on sharing real scenarios, lessons learned when building, operating and scaling AI systems on AWS.\nThrough the presentations, I better understood the role of architecture, security and automation in bringing ML models from the experimental stage to stable operation.\nPractical technical experience The presentation sessions helped me visualize the entire lifecycle of an ML system: From data preparation, training and tuning models To deployment and monitoring in cloud environment Additionally, the introduction to Generative AI provided a new perspective on how to combine Foundation Models with internal data, thereby creating intelligent applications capable of responding closely to business context. Modern tool application The workshop was an opportunity to directly learn about AWS AI/ML services: Amazon SageMaker simplifies the entire ML workflow Amazon Bedrock opens up the ability to build GenAI applications without managing complex infrastructure Through the demos, I realized that leveraging managed services helps technical teams focus more on logic and business value, rather than dealing with infrastructure issues. Connection and exchange The event created an open space for exchange between cloud, AI, DevOps practitioners and those in the learning phase. Sharing perspectives from different roles helped me understand that a successful AI system depends not only on the model but also on how the team coordinates and deploys. Side conversations brought more practical perspectives on learning and development paths in the AI/ML field on AWS. Lessons learned AI/ML and Generative AI only truly deliver value when placed in the context of specific problems Choosing appropriate services and models is as important as building the model Managed services on AWS significantly reduce barriers to deploying AI at scale Learning combined with practice is the fastest way to master new technology "},{"uri":"https://tqunhh.github.io/FCJ-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Takeaways from \u0026ldquo;DevOps on AWS Workshop\u0026rdquo; Event Purpose Equip end-to-end DevOps knowledge on AWS Build complete CI/CD pipeline Get familiar with and practice Infrastructure as Code (IaC) Deploy and operate container-based applications Properly understand monitoring \u0026amp; observability Share best practices, case studies and DevOps career path Speaker List Bao Huynh - AWS Community Builder Thinh Nguyen - AWS Community Builder Vi Tran - AWS Community Builder Highlighted Content CI/CD Pipeline with AWS DevOps Services Workshop went from foundation to practical deployment:\nSource Control\nAWS CodeCommit Compare GitFlow and Trunk-based development ‚Üí Clearly understand when to use each strategy for different team sizes and release models. Build \u0026amp; Test\nConfigure AWS CodeBuild Integrate unit test, integration test into pipeline Deployment\nBlue/Green deployment ‚Äì zero downtime Canary deployment ‚Äì controlled rollout Rolling updates ‚Äì suitable for stateless applications Demo Complete CI/CD pipeline was the biggest highlight, showing the entire flow from code commit to production deployment, along with rollback and error handling strategies.\nInfrastructure as Code (IaC) The IaC section helps change mindset from \u0026ldquo;click ops\u0026rdquo; to \u0026ldquo;infrastructure as software\u0026rdquo;.\nAWS CloudFormation\nTemplates, stack management Drift detection ‚Äì extremely important in production Nested stacks for complex architecture AWS CDK\nWrite infrastructure in TypeScript/Python instead of YAML/JSON Type safety, IDE support Reusable constructs and higher-level abstractions Testing infrastructure code Container Services on AWS Container content was presented very comprehensively:\nDocker Fundamentals Containerization concepts Microservices architecture Best practices for Dockerfile Multi-stage builds Amazon ECR Private container registry Image security scanning Lifecycle policies for image management Amazon ECS vs EKS ECS: AWS-native, simple setup, deep AWS integration EKS: Kubernetes standard, portable, higher learning curve Auto-scaling and deployment strategies for each service Monitoring \u0026amp; Observability One of the most practical and immediately applicable sections: Amazon CloudWatch Standard metrics \u0026amp; custom metrics Log aggregation Alarm with SNS notifications Dashboard design for each stakeholder AWS X-Ray Distributed tracing for microservices Service map visualization Performance bottleneck identification Observability Best Practices Three pillars: Metrics ‚Äì Logs ‚Äì Traces Alerting strategies to avoid alert fatigue On-call rotation \u0026amp; incident response DevOps Best Practices \u0026amp; Case Studies Progressive Delivery Feature flags A/B testing Canary analysis Automated Testing Test pyramid: Unit ‚Üí Integration ‚Üí E2E CI/CD integration Quality gates Incident Management Incident response procedures Blameless postmortems Learning from failures What I Learned After the workshop, I better understand: How to design production-ready CI/CD pipeline on AWS When to use Blue/Green, Canary or Rolling deployments IaC mindset and choosing the right tool for each stage Container deployment strategy suitable for workload Importance of observability in distributed systems DevOps is not just tooling but mindset: automation, collaboration, continuous improvement Personal Experience Overall Impression\nThe workshop was intensive and very practical. Compared to the previous AI/ML workshop, the DevOps workshop focused more on practical operations and immediately applicable skills.\nHighlights\nCI/CD pipeline demo with Blue/Green deployment AWS CDK ‚Äì truly a \u0026ldquo;game changer\u0026rdquo; Monitoring \u0026amp; observability section extremely practical Very relatable case studies Networking \u0026amp; Q\u0026amp;A\nDiscussion about career paths in DevOps AWS certification roadmap (DevOps Engineer ‚Äì Professional) Experience sharing from practitioners doing real DevOps "},{"uri":"https://tqunhh.github.io/FCJ-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Takeaways from \u0026ldquo;AWS Well-Architected Security Pillar Workshop\u0026rdquo; Event Purpose Provide comprehensive and systematic knowledge about security on AWS based on the 5 Security Pillars of AWS Well-Architected Framework Build security-first mindset, considering security as the foundation from the architecture design phase, not an afterthought. Guide practical best practices on: Introduce AI tools supporting development lifecycle Identity \u0026amp; Access Management (IAM) Logging, detection and continuous monitoring Infrastructure and network protection Data encryption, key and secrets management Incident response and forensics Speaker List Tran Duc Anh - AWS Cloud Club Captain (SGU) Tran Doan Cong Ly - AWS Cloud Club Captain (PTIT) Danh Hoang Hieu Nghi - AWS Cloud Club Captain (HUFLIT) Nguyen Tuan Thinh - Cloud Engineer Trainee Nguyen Do Thanh - Cloud Engineer Thinh Lam - FCJer Viet Nguyen - FCJer NMendel Grabski (Long) - ex Head of Security \u0026amp; DevOps - Cloud Security Solution Architect Quang Tinh Truongh - AWS Community Builder - Platform Engineer at TymeX Highlighted Content Identity \u0026amp; Access Management (IAM) ‚Äì foundation of security Design modern IAM with roles instead of users Apply least privilege principle Use IAM Identity Center (SSO) for multi-account model Manage and control permissions with SCP, permission boundaries and Access Analyzer Detection \u0026amp; Continuous Monitoring Deploy centralized logging with CloudTrail, VPC Flow Logs, CloudWatch Logs Detect threats with GuardDuty and aggregate alerts via Security Hub Automate alerting and incident response with EventBridge Infrastructure Protection Design secure network with VPC segmentation, private subnet and VPC endpoints Apply defense in depth model using Security Groups, NACLs Advanced protection with AWS WAF, Shield and Network Firewall Best practices for securing EC2, ECS/EKS and container workloads Incident Response \u0026amp; Automation Build Incident Response process according to AWS standards Practice real playbooks: IAM key exposure, public S3, EC2 with malware Collect forensics and maintain chain of custody Automate incident handling with Lambda, Step Functions What I Learned Correct security mindset from design Security is not an afterthought, but a foundational element in cloud architecture. Apply security-first mindset when designing, deploying and operating systems on AWS. Clearly understand the relationship between Security ‚Äì DevOps ‚Äì AI/ML, where security is the condition for sustainable system operation. Technical Architecture All components are tightly connected and continuously monitored, not dependent on a single control point. IAM plays the role of the first gateway of the entire architecture. Apply layered network security architecture. Event Experience Participating in the \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop gave me a very practical and comprehensive perspective on how to build security systems on AWS according to proper architectural standards. Although it only took place in the morning, the workshop still had high knowledge density and focused precisely on core issues that technical teams often encounter in practice.\nLearning from highly specialized speakers Speakers from AWS Community, Cloud Security and real enterprises shared security-first perspectives in cloud system design and operation. Through examples close to production environments, I better understood how security best practices are applied in enterprises, rather than just stopping at theory or checklists. Practical technical experience Workshop content was presented according to the 5 Security Pillars of AWS Well-Architected Framework, helping me have a systematic view of end-to-end security architecture. Sections on IAM, logging, detection and incident response helped me better visualize how security components connect with each other in real architecture. Especially, examples of incident scenarios (IAM key exposure, public S3, EC2 infected with malware) helped me understand incident response processes instead of just \u0026ldquo;knowing services\u0026rdquo;. Modern tool application Directly learn how to combine services like CloudTrail, GuardDuty, Security Hub and EventBridge to build continuous detection and alerting systems. Clearly understand the role of AWS KMS, Secrets Manager in protecting data and managing secrets securely. Recognize the potential of automation in security, from auto-remediation with Lambda to orchestrating workflows with Step Functions. Connection and exchange The workshop created an open environment to exchange with experienced security and cloud engineering professionals, helping me learn many lessons beyond slides. Through real sharing, I realized security is not just the responsibility of one team, but requires close coordination between engineering, operations and business. Lessons learned Security must start from architecture, cannot patch holes at the final stage IAM and logging are the two most important foundations, if done wrong will lead to many risks later. Incident Response needs to be prepared in advance with playbooks and automation, cannot be handled effectively if only reacting when incidents occur. Using AWS managed services helps reduce operational burden and limit human errors in security. "},{"uri":"https://tqunhh.github.io/FCJ-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Truc Quynh\nPhone Number: 0909763550\nEmail: quynhntse180280@fpt.edu.vn\nUniversity: FPT University HCM\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Route 53 and AWS Backup\nWeek 3: Storage services (S3, Storage Gateway)\nWeek 4: Storage Gateway and VM migration\nWeek 5: Security services (IAM, KMS, Security Hub)\nWeek 6: Frontend learning and architecture design\nWeek 7: Midterm exam preparation\nWeek 8: Midterm exam and project setup\nWeek 9: Homepage creation and API integration\nWeek 10: AWS Cloud Mastery event and proposal writing\nWeek 11: Frontend restructuring and AWS Cloud Mastery #2\nWeek 12: Rekognition testing and final preparations\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.1-week1/","title":"Worklog Week 1","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with First Cloud Journey members. Understand basic AWS services, how to use console \u0026amp; CLI. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Form team - Read and memorize company regulations - Learn what cloud computing is 08/09/2025 08/09/2025 3 - Create and configure AWS account - Practice using AWS Console and AWS CLI - Setup MFA device 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/ 4 - Create admin group and admin user - Create budgets to manage account - Learn and create Amazon VPC, Subnet, Internet Gateway, Route table, Security Group,\u0026hellip; 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/3-create-admin-user-and-group/ 5 - Learn EC2 basics, instance types Hands-on: - Create EC2 server, check connection, create NAT Gateway - Create EC2 instance connect endpoint 11/09/2025 11/09//2025 https://000003.awsstudygroup.com/3-prerequisite/ 6 - Hands-on: + Create EC2 instance + SSH connection - Learn about Route 53 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/4-createec2server/ Week 1 Achievements: Understand what AWS is and grasp basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured AWS Free Tier account.\nFamiliarized with AWS Management Console\nUser and cost management\nCreate and manage IAM User, Group and assign permissions for secure access Set up Budget to monitor and control costs Install and configure AWS CLI on computer including:\nAccess Key Secret Key \u0026hellip; Understand and deploy networking on AWS\nGrasp concepts and configure Amazon VPC Know how to create Subnet, Internet Gateway, Route table, Security Groups Hands-on with EC2\nUnderstand EC2 instance types and use cases Practice creating EC2 instance, connecting via SSH/Session Manager Create and configure NAT Gateway Manage EC2 security through Security Groups "},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand the role of Route 53. Deploy AWS Backup. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Learn Hybrid DNS concept - Set up DNS infrastructure + Create Route 53 Outbound Endpoint + Create Route 53 Inbound Endpoint + Configure Resolver Rules 15/09/2025 15/09/2025 https://000010.awsstudygroup.com/vi/ 3 - Learn and set up VPC Peering - Understand why to use peering instead of NAT/Internet gateway - Create CloudFormation template, Security Group, EC2 - Update Network ACL - Create peering connection and update route tables 16/09/2025 16/09/2025 https://000019.awsstudygroup.com/vi/ 4 - Learn about Transit Gateway concept and advantages - Hands-on: + Create Transit Gateway + Transit Gateway Attachments + Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 17/09/2025 17/09/2025 https://000020.awsstudygroup.com/vi/ 5 - Learn how to create, manage, store, automate and scale EC2 Instance on AWS - Distinguish between EBS volume and Instance Store - How to automatically configure EC2 at initialization using User Data Script - Learn how to deploy auto scaling system 18/09/2025 18/09/2025 6 - Hands-on: + Create S3 bucket to store backup data + Deploy infrastructure + Create Backup Plan 19/09/2025 19/09/2025 https://000013.awsstudygroup.com/vi/ Week 2 Achievements: Understand Hybrid DNS concept:\nRole of Route 53 Resolver when handling internal \u0026amp; external domain names Components in hybrid DNS: Outbound Endpoint, Inbound Endpoint, Resolver Rules Distinguish two types of storage:\nEBS (Elastic Block Store) ‚Äì virtual disk attached to instance. Instance Store ‚Äì temporary disk, lost when machine is turned off. Transit Gateway operates as a \u0026ldquo;central router\u0026rdquo; between multiple VPCs (and possibly on-prem)\nHow to create Amazon S3 Bucket to store data on cloud.\nSet up Bucket Policy, Permissions, and Public Access Block.\nDeploy AWS Backup:\nCreate Backup Vault to store backups Create Backup Plan and schedule automatic backups. Assign resources (EC2, EBS, RDS, EFS, ‚Ä¶) to backup plan Test and try restoring backed up data. "},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Learn about storage service concepts Practice creating Amazon services Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Deploy File Storage Gateway - Set up File Sharing - Hands-on: + Create Storage Gateway + Create File Shares via gateway + Connect File Shares from On-premise machine (Windows, Linux) via SMB/NFS 22/09/2025 22/09/2025 https://000024.awsstudygroup.com/vi/ 3 Hands-on: + Create S3 bucket + Upload data to bucket + Enable static website feature + Configure Block Public Access + Configure Public Object + Check website 23/09/2025 23/09/2025 https://000057.awsstudygroup.com/vi/ 4 Hands-on: + Speed up with CloudFront + Block direct public access to S3 + Configure CloudFront distribution to serve files + Manage versions ‚Äî restore if deleted by mistake + Move Object + Copy Object to another region 24/09/2025 24/09/2025 https://000057.awsstudygroup.com/vi/ 5 - Overview of AWS storage services + Amazon S3 + EBS (Elastic Block Store) + EFS (Elastic File System) + Storage Gateway + Snow Family 25/09/2025 25/09/2025 6 - Learn the main structure of bucket, object and object key in S3 - Deploy static website with S3 - Control access via Bucket Policy, ACL, IAM - Introduction to S3 Glacier - Learn about Snow Family 26/09/2025 25/09/2025 Week 3 Achievements: Deploy File Storage Gateway:\nKnow how to operate Storage Gateway to integrate on-premise file storage with S3 Set up file share via SMB / NFS through gateway Practice connecting from client machine to gateway Hands-on with Amazon S3:\nKnow how to create and configure S3 bucket, upload/download data. Set up access permissions and host static website on S3. Know backup tools and data migration in AWS. Understand concepts and differences between storage services:\nAmazon S3: object storage. EBS: block storage for EC2. Instance Store: temporary storage on EC2 server\u0026rsquo;s physical hard drive. Amazon EFS ‚Äì shared file system AWS Backup, Storage Gateway, Snow Family: backup and data migration. "},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.4-week4/","title":"Worklog Week 4","tags":[],"description":"","content":"Week 4 Objectives: Learn about Storage Gateway Learn how to migrate virtual machines (VMs) from on-premises environment to AWS Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Hands-on: + Create S3 bucket to store backup data + Deploy infrastructure + Create Backup Plan + Set up notifications 29/09/2025 29/09/2025 https://000013.awsstudygroup.com/vi/ 3 - Prepare virtual machine + Install VMware Workstation Pro 17 + Download Ubuntu operating system - Hands-on + Export VM from On-premise + Upload VM to AWS + Import VM into AWS + Deploy EC2 from AMI 30/09/2025 30/09/2025 https://000014.awsstudygroup.com/vi/ 4 - Hands-on: + Set up ACL for S3 Bucket + Export VM from EC2 + Clean up resources 01/10/2025 01/10/2025 https://000014.awsstudygroup.com/vi/ 5 - Hands-on: + Create S3 Bucket and EC2 for Storage Gateway + Create Storage Gateway + Connect File shares at On-premise 02/10/2025 02/10/2025 https://000024.awsstudygroup.com/vi/ 6 - Learn about Amazon FSx for Windows File Server - How to create new file share on FSx - Check file share performance - Learn advanced features in FSx 03/10/2025 03/10/2025 https://000025.awsstudygroup.com/ Week 4 Achievements: Understand Storage Gateway:\nConfigure File Gateway Create file share (SMB / NFS) via Gateway Mount share from client machine (on-premise or EC2) Manage file sharing and users Learn how to migrate virtual machines (VMs) from on-premises environment to AWS to run on EC2:\nUse AWS VM Import/Export service. Create S3 Bucket \u0026amp; Upload File Create IAM Role for Import Perform Import Understand the process of creating, uploading, importing, and creating EC2 from AMI Understand Amazon FSx for Windows File Server:\nHow to create new file share on FSx, via Windows interface or share management tools. Check file share performance Manage file shares: open new file share, grant permissions, manage user sessions, open files. "},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Objectives: Connect and get acquainted with First Cloud Journey members. Understand basic AWS services, how to use console \u0026amp; CLI. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Learn about: + Amazon Cognito + AWS Organization + AWS Identity Center + Amazon Key Management Service + AWS Security Hub 06/10/2025 06/08/2025 3 - Hands-on: + Enable Security Hub + View \u0026amp; evaluate results + Clean up resources - Discuss with team about project topic 07/10/2025 07/10/2025 https://000018.awsstudygroup.com/ 4 - Prepare practice environment: create VPC, create SG,\u0026hellip; - Hands-on: + Assign tags to EC2 instance + Create IAM Role for Lambda + Create 2 Lambda functions + Set up automation schedule + Check \u0026amp; verify operation 08/10/2025 08/10/2025 https://000022.awsstudygroup.com/ 5 - Hands-on: + Use tags through Console interface + Use tags via CLI / command line + Create a Resource Group - Continue team meeting about project topic, then finalize project and product features 09/10/2025 09/10/2025 https://000027.awsstudygroup.com/ 6 - Hands-on: + Prepare: create IAM user + Create IAM Policy + Create IAM Role + Check policy / access permissions + Switch to created role, try accessing EC2 console in different regions + Try creating EC2 instance ‚Äî only succeed if instance has valid tag 10/10/2025 10/10/2025 https://000028.awsstudygroup.com/ Week 5 Achievements: Understand and grasp identity \u0026amp; security management services on AWS, including:\nAmazon Cognito AWS Organizations AWS Identity Center Amazon Key Management Service (KMS) AWS Security Hub Practice AWS Security Hub and grasp basic usage process:\nEnable Security Hub for AWS account View dashboard and evaluate security results, compliance level Practice automating EC2 management with Lambda\nGrasp how to manage resources using Tag \u0026amp; Resource Group\nUse tags through AWS Management Console Use tags via AWS CLI / command line Practice managing EC2 access based on Resource Tag with IAM\nCreate IAM User for permission assignment Create IAM Policy with tag checking conditions Create IAM Role and perform switch role \u0026hellip; Conduct team meeting, then select project topic and main product features\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives: Learn about Frontend Create basic architecture Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Hands-on: + Create \u0026amp; configure key (CMK) in KMS + Create S3 bucket and upload data + Log \u0026amp; set up audit + Test \u0026amp; experiment sharing encrypted data - Proceed to write official proposal for project 13/10/2025 13/10/2025 https://000033.awsstudygroup.com/ 3 Hands-on: + Create IAM Group, IAM Users \u0026amp; Create an IAM Role + Configure conditions for Role + Switch Role \u0026amp; check + Check permissions of other Users/Groups - Design and divide infrastructure architecture for project. - Draw first architecture diagram. - Discuss with team to develop architecture 14/10/2025 14/10/2025 https://000044.awsstudygroup.com/ 4 - Learn Frontend basics - Think roughly about web interface - Proceed to modify old architecture 15/10/2025 15/10/2025 5 - Learn about React, HTML, CSS,\u0026hellip; - Attend sharing session on DevSecOps - Reinventing DevSecOps with AWS Generative AI 16/10/2025 16/10/2025 6 - Review for midterm exam - Continue meeting to modify architecture, add missing services 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Practice AWS KMS \u0026amp; S3:\nCreate and configure Customer Managed Key (CMK) in KMS Create S3 bucket, upload and encrypt data Log, set up audit and check secure encrypted data sharing Practice advanced IAM:\nCreate IAM Group, IAM User and IAM Role Configure Condition for Role, perform Switch Role and check permissions Verify access permissions of other Users/Groups Complete and write official proposal for project\nParticipate in designing project infrastructure architecture:\nDivide architecture, draw initial architecture diagram Discuss, adjust and improve architecture with team Learn Frontend basics:\nGrasp foundational knowledge of React, HTML, CSS Come up with web interface ideas Attend DevSecOps sharing session ‚Äì Reinventing DevSecOps with AWS Generative AI\nReview and prepare for midterm exam\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Focus on reviewing for midterm exam Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Review knowledge, prepare for midterm exam - Review security architecture + IAM, MFA, SCP, Encryption, Security Group,\u0026hellip; 20/10/2025 20/10/2025 3 - Edit proposal for better completion - Continue reviewing for midterm exam: + Design flexible and resilient architecture + Auto Scaling + Route 53 + \u0026hellip; 21/10/2025 21/10/2025 4 - Consolidate Networking knowledge (VPC, Subnets, Route Tables, Internet Gateway, Security Group) - Review Database (RDS, DynamoDB) - Practice sample exams on web 22/10/2025 22/10/2025 5 - Changes in architecture design ideas, meet to add more services - Continue exam review, review cost optimization design section 23/10/2025 23/10/2025 6 - Take practice exams on AWSBOY Web - Reference and read more materials on AWS Builders 24/10/2025 24/10/2025 Week 7 Achievements: Review knowledge to prepare for midterm exam, focusing on:\nAWS Security Architecture: IAM, MFA, SCP, Encryption, Security Group Design flexible, resilient architecture, optimize availability Important services: Auto Scaling, Route 53 Consolidate AWS Networking knowledge: VPC, Subnet, Route Table, Internet Gateway, Security Group\nReview and compare Database solutions: RDS and DynamoDB\nEdit and complete project proposal; update architecture when there are idea changes, add appropriate AWS services:\nPractice taking sample exams on AWSBOY Web and online practice sites\nReference and read more in-depth materials from AWS Builders to enhance design thinking and cost optimization\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Week 8 Objectives: Midterm exam Initial project setup Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Comprehensive review of knowledge to prepare for midterm exam - Practice sample exams 27/10/2025 27/10/2025 3 - Proceed to design project login/register page - Experiment with many different login/register styles - Discuss with team about project style 28/10/2025 28/10/2025 4 - Team meeting to review together, solve problems together to prepare for midterm exam - Setup project development environment 29/10/2025 29/10/2025 5 - Edit login/register page, add effects for liveliness - Final review before exam day 30/10/2025 30/10/2025 6 - Midterm exam at AWS office - Consolidate Serverless knowledge 31/10/2025 31/10/2025 Week 8 Achievements: Comprehensive review with team to prepare for exam\n31/10 midterm exam at AWS office\nDesign login/register page\nProceed to select project style with team\nConsolidate knowledge of important Serverless services (Lambda, DynamoDB) for project development\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives: Create Homepage. Call API for some basic web functions. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Come up with homepage design ideas - Choose main color for web - Design buttons displayed on web 03/11/2025 03/11/2025 3 - Design post creation page - Create input fields such as: + Select image + Enter caption + Select emoji + Select and enter location 04/11/2025 04/11/2025 4 - Call API from Backend for some basic post functions: + create + update + delete + edit 05/11/2025 05/11/2025 5 - Edit post creation page for better completion - Call API from 3rd party - OpenStreetMap into web 06/11/2025 06/11/2025 6 - Complete location search function - Attach marker on map pointing to correct user post location 07/11/2025 07/11/2025 Week 9 Achievements: Complete frontend interface ideas and design:\nLayout homepage, choose main color for website Design buttons and basic UI components Design and complete post creation page:\nBuild input fields: select image, enter caption, select emoji, select/enter location Connect Frontend with Backend:\nCall API to handle post functions: create, update, edit, delete Integrate third-party OpenStreetMap service into system Display map and attach marker correctly to user post location "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.1-workshop-overview/","title":"5.1 Overview of the Workshop","tags":[],"description":"","content":"Introduction This workshop provides an overview of the Travel Journal Web Application built during the project.\nThe main goal is to help learners understand how to design and deploy a modern web system using a serverless architecture on AWS, integrating services such as AWS Rekognition, AWS Location Service, DynamoDB, and Amazon S3 to create a scalable, cost-optimized, and practical product.\nThe platform allows users to record their travel experiences by uploading photos, tagging locations, creating posts, and sharing personal journeys.\nWhen a photo is uploaded, Amazon Rekognition analyzes its content and automatically assigns labels.\nMetadata of each post is saved to DynamoDB, images are stored in S3, and static content is delivered through Amazon CloudFront to ensure fast performance globally.\nKey Feature ‚Äì Location Mapping A unique highlight of the system is the ability to visualize user travel routes using AWS Location Service, displaying the geographic points of each post on an interactive map.\nServerless Backend Architecture The backend is fully built on a serverless stack:\nAWS Lambda ‚Äì handles business logic API Gateway ‚Äì exposes REST APIs DynamoDB ‚Äì stores post and user metadata S3 + SQS + Lambda pipeline ‚Äì processes uploaded images Rekognition ‚Äì automatic labeling CloudFront ‚Äì high-performance content delivery This architecture eliminates server management and automatically scales based on workload.\nFrontend Implementation The frontend is a React-based SPA deployed on either:\nAWS CloudFront ensuring easy deployment and fast global access.\nWorkshop Goal By the end of this workshop, learners will understand:\nHow to integrate multiple AWS services to build a complete web system How serverless architecture works How to process media data with AI Rekognition How to deploy, secure, and evaluate system performance "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.6-cloudfront-s3-location/5.6.2-cloudfront-cdn/","title":"CloudFront CDN Configuration","tags":[],"description":"","content":"Amazon CloudFront (Static + Dynamic) In the Travel Guide project, CloudFront serves as the CDN for:\nStatic web (React build) from StaticSiteBucket Images from ArticleImagesBucket with path patterns The \u0026ldquo;dynamic\u0026rdquo; part (API) currently runs directly through API Gateway URL. This workshop describes (1) deployed static setup + (2) recommended dynamic model if you want to route API through CloudFront.\nArchitecture Overview The system uses CloudFront as a unified entry point for both static content and media files, providing:\nGlobal content delivery HTTPS enforcement Compression Caching optimization CloudFront Static ‚Äî Host React from Private S3 Origin 1: S3 StaticSiteBucket (Private) Configuration:\nAccess via Origin Access Identity (OAI) DefaultRootObject: index.html ViewerProtocolPolicy: redirect-to-https Compression enabled Origin Access Identity (OAI) Setup OAI allows CloudFront to access private S3 buckets securely without making them public.\nS3 Bucket Policy for OAI:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontOAI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity \u0026lt;OAI-ID\u0026gt;\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::static-site-bucket/*\u0026#34; } ] } Deploying Frontend Build and Upload cd travel-guide-frontend npm install npm run build # Upload build/ to StaticSiteBucket aws s3 sync build/ s3://\u0026lt;StaticSiteBucketName\u0026gt;/ --delete Invalidation After Deploy After each frontend deployment, create an invalidation so users receive the latest version:\naws cloudfront create-invalidation \\ --distribution-id \u0026lt;DISTRIBUTION_ID\u0026gt; \\ --paths \u0026#34;/*\u0026#34; Best Practice:\nInvalidate only changed paths to save costs Use versioned filenames (e.g., app.v123.js) to avoid invalidation Invalidate index.html and other non-hashed files CloudFront for Images ‚Äî Cache and Accelerate Media Loading Origin 2: S3 ArticleImagesBucket (Private) Cache behaviors by path:\narticles/* thumbnails/* avatars/* covers/* Cache Behaviors Configuration When frontend needs to display images:\nConstruct URL like: https://\u0026lt;cloudfront-domain\u0026gt;/\u0026lt;key\u0026gt; Example: https://d1zx7zcxy3hbwd.cloudfront.net/articles/... Benefits:\nImages cached at edge locations globally Reduced S3 GET requests Faster load times for users Automatic HTTPS Cache Strategy Recommendations Static Web index.html:\nTTL: Low (0-60 seconds) or use custom cache-control Reason: Ensure users get latest app version Hashed assets (js/css):\nTTL: High (1 week - 1 year) Reason: Filenames change with content, safe to cache long-term Example Cache-Control Headers:\n# index.html Cache-Control: no-cache, must-revalidate # app.abc123.js Cache-Control: public, max-age=31536000, immutable Images Configuration:\nTTL: High (images rarely change) If image changes with same key ‚Üí change key (best practice) or invalidate Recommended TTL:\nMin: 1 day Default: 7 days Max: 1 year CloudFront Dynamic ‚Äî Recommended Model (Optional) If you want \u0026ldquo;dynamic\u0026rdquo; (API) to run through CloudFront, add:\nOrigin 3: API Gateway Endpoint (Custom Origin) Configuration:\nBehavior: /api/* (or /Prod/*) ‚Üí API origin Cache: Disabled or very low TTL Forward headers: Authorization + query strings Forward cookies: As needed Goals:\nSingle domain for both web + API Reduce global latency for API (depending on use case) Note: Configuring API through CloudFront requires proper path normalization and CORS setup. Also consider caching strategy to avoid \u0026ldquo;wrong cache\u0026rdquo; based on tokens.\nMonitoring and Metrics CloudWatch Metrics Key metrics to monitor:\nCache Hit Ratio\nTarget: \u0026gt; 80% for static content Low ratio indicates caching issues 4xx Error Rate\nCommon: 403 (access denied), 404 (not found) Check OAI permissions and file paths 5xx Error Rate\nOrigin errors Check S3 availability Bytes Downloaded\nTrack bandwidth usage Identify popular content Requests\nTotal requests per time period Identify traffic patterns Operations \u0026amp; Troubleshooting Common Issues 1. 403 Access Denied Causes:\nOAI not configured correctly S3 bucket policy missing CloudFront permission Object doesn\u0026rsquo;t exist Solution:\nVerify OAI ID in bucket policy Check object exists in S3 Test direct S3 access (temporarily) 2. Stale Content After Deploy Causes:\nNo invalidation created Browser cache Solution:\nCreate CloudFront invalidation Use versioned filenames Set appropriate Cache-Control headers 3. Slow First Load Cause:\nCold cache (first request to edge location) Expected behavior:\nFirst request: Slower (origin fetch) Subsequent requests: Fast (cached) 4. High Origin Requests Causes:\nLow cache hit ratio TTL too short Query strings not handled properly Solution:\nIncrease TTL for static content Configure query string forwarding Use cache key normalization Security Best Practices HTTPS Only\nRedirect HTTP to HTTPS Use TLS 1.2 minimum Origin Access Identity\nNever make S3 buckets public Use OAI for CloudFront access Geo Restrictions (Optional)\nWhitelist/blacklist countries if needed AWS WAF Integration (Optional)\nProtect against common web exploits Rate limiting Signed URLs/Cookies (Optional)\nFor premium/private content Time-limited access Cost Optimization Strategies:\nHigh Cache Hit Ratio\nReduces origin requests Lower S3 GET costs Compression\nReduces data transfer Faster load times Price Class Selection\nUse fewer edge locations if users are regional Price Class 100 (US, Europe, Asia) Reserved Capacity (High traffic)\nContact AWS for discounts Monitor Invalidations\nFirst 1,000 paths/month free Use versioned filenames instead Key Takeaways CloudFront accelerates content delivery globally OAI secures private S3 access Cache behaviors optimize different content types Invalidation ensures users get latest content Monitoring helps identify and resolve issues Proper TTL balances freshness and performance "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.7-security/5.7.2-input-sanitization/","title":"Input Sanitization","tags":[],"description":"","content":"Input Sanitization - XSS Prevention What is Input Sanitization? Input Sanitization is the process of cleaning and validating user input to prevent malicious code injection.\nCommon attacks prevented:\nXSS (Cross-Site Scripting) - Injecting JavaScript SQL/NoSQL Injection - Manipulating database queries Path Traversal - Accessing unauthorized files Command Injection - Executing system commands Implementation Overview We created a comprehensive security utilities module with 9 functions to sanitize and validate all user inputs.\nFile Created: travel-guide-backend/shared/layers/common/python/security_utils.py\nSecurity Functions 1. HTML Sanitization Function: sanitize_html(text)\nPurpose: Escape HTML entities to prevent XSS attacks\nBefore:\ntitle = data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;).strip() # ‚ùå Vulnerable: \u0026lt;script\u0026gt;alert(\u0026#39;XSS\u0026#39;)\u0026lt;/script\u0026gt; After:\nfrom security_utils import sanitize_html title = sanitize_html(data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;)) # ‚úÖ Safe: \u0026amp;lt;script\u0026amp;gt;alert(\u0026#39;XSS\u0026#39;)\u0026amp;lt;/script\u0026amp;gt; Implementation:\nimport html def sanitize_html(text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Escape HTML entities to prevent XSS\u0026#34;\u0026#34;\u0026#34; if not text: return \u0026#34;\u0026#34; # Escape HTML special characters sanitized = html.escape(text) # Remove null bytes sanitized = sanitized.replace(\u0026#39;\\x00\u0026#39;, \u0026#39;\u0026#39;) return sanitized.strip() 2. String Sanitization Function: sanitize_string(text, max_length=1000)\nPurpose: Clean and validate string inputs\ndef sanitize_string(text: str, max_length: int = 1000) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Sanitize general string input\u0026#34;\u0026#34;\u0026#34; if not text: return \u0026#34;\u0026#34; # Remove control characters sanitized = \u0026#39;\u0026#39;.join(char for char in text if char.isprintable() or char.isspace()) # Trim whitespace sanitized = sanitized.strip() # Enforce max length if len(sanitized) \u0026gt; max_length: sanitized = sanitized[:max_length] return sanitized 3. Coordinate Validation Function: validate_coordinates(latitude, longitude)\nPurpose: Ensure coordinates are valid\ndef validate_coordinates(latitude: float, longitude: float) -\u0026gt; tuple: \u0026#34;\u0026#34;\u0026#34;Validate geographic coordinates\u0026#34;\u0026#34;\u0026#34; try: lat = float(latitude) lng = float(longitude) # Check ranges if not (-90 \u0026lt;= lat \u0026lt;= 90): raise ValueError(\u0026#34;Latitude must be between -90 and 90\u0026#34;) if not (-180 \u0026lt;= lng \u0026lt;= 180): raise ValueError(\u0026#34;Longitude must be between -180 and 180\u0026#34;) return (lat, lng) except (ValueError, TypeError) as e: raise ValueError(f\u0026#34;Invalid coordinates: {e}\u0026#34;) 4. Email Validation Function: validate_email(email)\nPurpose: Validate email format\nimport re def validate_email(email: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Validate email address format\u0026#34;\u0026#34;\u0026#34; if not email: raise ValueError(\u0026#34;Email is required\u0026#34;) # Basic email regex pattern = r\u0026#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\u0026#39; if not re.match(pattern, email): raise ValueError(\u0026#34;Invalid email format\u0026#34;) return email.lower().strip() 5. S3 Key Validation Function: validate_s3_key(key)\nPurpose: Prevent path traversal attacks\ndef validate_s3_key(key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Validate S3 object key to prevent path traversal\u0026#34;\u0026#34;\u0026#34; if not key: raise ValueError(\u0026#34;S3 key is required\u0026#34;) # Check for path traversal attempts if \u0026#39;..\u0026#39; in key or key.startswith(\u0026#39;/\u0026#39;): raise ValueError(\u0026#34;Invalid S3 key: path traversal detected\u0026#34;) # Check for null bytes if \u0026#39;\\x00\u0026#39; in key: raise ValueError(\u0026#34;Invalid S3 key: null byte detected\u0026#34;) return key.strip() 6. Article Ownership Validation Function: validate_article_ownership(article, user_id)\nPurpose: Ensure user owns the article\ndef validate_article_ownership(article: dict, user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate that user owns the article\u0026#34;\u0026#34;\u0026#34; if not article: raise ValueError(\u0026#34;Article not found\u0026#34;) article_owner = article.get(\u0026#39;owner_id\u0026#39;) or article.get(\u0026#39;ownerId\u0026#39;) if article_owner != user_id: raise PermissionError(\u0026#34;You don\u0026#39;t have permission to modify this article\u0026#34;) return True 7. Tag Sanitization Function: sanitize_tags(tags)\nPurpose: Validate and limit tags\ndef sanitize_tags(tags: list) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34;Sanitize and validate tags\u0026#34;\u0026#34;\u0026#34; if not tags: return [] # Limit number of tags MAX_TAGS = 10 if len(tags) \u0026gt; MAX_TAGS: tags = tags[:MAX_TAGS] # Sanitize each tag sanitized_tags = [] for tag in tags: if isinstance(tag, str): # Clean tag clean_tag = sanitize_string(tag, max_length=50) if clean_tag: sanitized_tags.append(clean_tag) return sanitized_tags 8. Image Key Validation Function: validate_image_key(key, article_id, owner_id)\nPurpose: Validate image ownership\ndef validate_image_key(key: str, article_id: str, owner_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate that image belongs to article and user\u0026#34;\u0026#34;\u0026#34; # Check S3 key format validate_s3_key(key) # Check if key starts with correct article path expected_prefix = f\u0026#34;articles/{article_id}/\u0026#34; if not key.startswith(expected_prefix): raise PermissionError(\u0026#34;Image does not belong to this article\u0026#34;) return True 9. Rate Limiting Key Function: rate_limit_key(user_id, action)\nPurpose: Generate key for rate limiting\ndef rate_limit_key(user_id: str, action: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate rate limit key for user action\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;rate_limit:{user_id}:{action}\u0026#34; Integration with Lambda Functions Updated: create_article.py Before (Vulnerable):\ndef lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) # ‚ùå No sanitization title = data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;).strip() content = data.get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;).strip() tags = data.get(\u0026#34;tags\u0026#34;, []) # Store directly in DynamoDB # ‚Üí XSS vulnerable! After (Secure):\nfrom security_utils import ( sanitize_html, sanitize_tags, validate_coordinates, validate_image_key ) def lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] # ‚úÖ Sanitize all inputs title = sanitize_html(data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;)) content = sanitize_html(data.get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;)) tags = sanitize_tags(data.get(\u0026#34;tags\u0026#34;, [])) # ‚úÖ Validate coordinates lat, lng = validate_coordinates( data.get(\u0026#34;latitude\u0026#34;), data.get(\u0026#34;longitude\u0026#34;) ) # ‚úÖ Validate image ownership for image_key in data.get(\u0026#34;imageKeys\u0026#34;, []): validate_image_key(image_key, article_id, user_id) # Now safe to store Testing Test XSS Protection # Attempt XSS attack curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;\u0026lt;script\u0026gt;alert(\\\u0026#34;XSS\\\u0026#34;)\u0026lt;/script\u0026gt;\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;img src=x onerror=alert(1)\u0026gt;\u0026#34;, \u0026#34;latitude\u0026#34;: 10.8231, \u0026#34;longitude\u0026#34;: 106.6297 }\u0026#39; # Expected: HTML escaped # Title: \u0026#34;\u0026amp;lt;script\u0026amp;gt;alert(\\\u0026#34;XSS\\\u0026#34;)\u0026amp;lt;/script\u0026amp;gt;\u0026#34; # Content: \u0026#34;\u0026amp;lt;img src=x onerror=alert(1)\u0026amp;gt;\u0026#34; Test Coordinate Validation # Invalid coordinates curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;latitude\u0026#34;: 999, \u0026#34;longitude\u0026#34;: -999 }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Invalid coordinates: Latitude must be between -90 and 90\u0026#34; Test Tag Limits # Too many tags curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;tags\u0026#34;: [\u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34;, ..., \u0026#34;tag20\u0026#34;] }\u0026#39; # Expected: Only first 10 tags saved Validation Rules Title Max length: 200 characters HTML escaped No control characters Content Max length: 10,000 characters HTML escaped No control characters Tags Max count: 10 tags Max length per tag: 50 characters Sanitized strings Coordinates Latitude: -90 to 90 Longitude: -180 to 180 Must be numbers Images Max size: 10 MB Allowed types: JPEG, PNG, GIF, WebP Ownership validated Best Practices 1. Sanitize at Entry Point ‚úÖ Do: Sanitize in Lambda handler\nBefore processing Before storing in database ‚ùå Don\u0026rsquo;t: Sanitize in frontend only\nCan be bypassed Not secure 2. Validate Everything ‚úÖ Do: Validate all user inputs\nType checking Range checking Format validation 3. Use Allowlists ‚úÖ Do: Define allowed values\nAllowed file types Allowed characters Allowed ranges ‚ùå Don\u0026rsquo;t: Use blocklists\nEasy to bypass Incomplete protection 4. Escape Output ‚úÖ Do: Escape when displaying\nHTML escape in templates URL encode in links JSON encode in APIs Key Takeaways Input sanitization prevents XSS and injection attacks HTML escaping is essential for user-generated content Validation should happen server-side Security utils provide reusable sanitization functions All inputs must be validated - never trust user data Defense in depth - sanitize at multiple layers "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.3-backend-articles/5.3.2-dynamotable/","title":" DynamoDB Tables Overview","tags":[],"description":"","content":"5.4 Overview of DynamoDB Tables This section explains all DynamoDB tables defined inside the backend architecture.\nEach table serves a specific role in powering article management, user interactions, gallery rendering, location services, and user profiles.\nComplete DynamoDB Tables Architecture 1. ArticlesTable Purpose:\nStores all articles created by users. Each record represents a complete travel post including metadata, visibility settings, timestamps, and ownership.\nPrimary Key:\narticleId (HASH) Attributes:\nvisibility ‚Äî public/private createdAt ‚Äî ISO timestamp ownerId ‚Äî ID of the user who created the article status ‚Äî draft/published/flagged Indexes:\ngsi_visibility_createdAt (visibility, createdAt) ‚Üí For listing public articles in the home feed. gsi_owner_createdAt (ownerId, createdAt) ‚Üí For user profile pages. gsi_status_createdAt (status, createdAt) ‚Üí For admin filtering and moderation. 2. UserFavoritesTable Purpose:\nStores the relationship between a user and the articles they have favorited.\nPrimary Key (Composite):\nuserId (HASH) articleId (RANGE) This structure allows efficient:\nChecking whether a user has favorited an article Listing all favorites of the current user 3. GalleryPhotosTable Purpose:\nStores metadata for individual images extracted from articles.\nUsed for gallery browsing, filtering by tags, and building trending metrics.\nPrimary Key:\nphoto_id (HASH) Metadata may include:\narticleId ‚Äî which article the photo belongs to S3 image key detected tags geolocation or timestamps 4. GalleryTrendsTable Purpose:\nMaintains tag popularity statistics for the gallery.\nThe article/image ingestion process updates counts within this table.\nPrimary Key:\ntag_name (HASH) Usage:\nProvides trending tags for the homepage Enables tag-based article recommendation 5. UserProfilesTable Purpose:\nStores public profile information of users displayed in article feeds and profile pages.\nPrimary Key:\nuserId (HASH) Attributes include:\navatar image URL full name bio/description This table keeps profile data separate from Cognito, allowing customization and public display without exposing private identity data.\n6. LocationCacheTable Purpose:\nCaches results from AWS Location Service\u0026rsquo;s reverse-geocoding to reduce cost and improve performance.\nPrimary Key:\ngeohash (HASH) Attributes:\nhuman-readable location TTL ‚Äî automatically deletes outdated entries Summary Together, these DynamoDB tables form the storage foundation for:\nArticle CRUD Favorites Gallery features User profile rendering Location lookup optimization They are optimized for serverless workloads with high scalability and low latency.\n"},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.5-auth-cognito-iam/5.5.2-iam-roles-policies/","title":"IAM Roles &amp; Policies","tags":[],"description":"","content":"IAM - Identity \u0026amp; Access Management Overview IAM is the service for managing access permissions (identity \u0026amp; access). In the Travel Guide workshop, IAM is used to:\nCreate users/groups for developers (standard credentials, MFA) Create roles for Lambda, ECS, EC2 with necessary permissions (least privilege) Create policies allowing Lambda to access S3/DynamoDB/CloudWatch/Cognito Admin Configure trust policies (who can assume roles) Grant API Gateway permission to invoke Lambda (resource-based permission) (Optional) Create roles/permissions for CI/CD What is IAM used for in our project? In the Travel Guide architecture:\nLambda reads/writes DynamoDB Lambda uploads images to S3 Lambda calls Rekognition to analyze images Lambda writes logs to CloudWatch Preparation Steps AWS Account Requirements Prerequisites:\n1 AWS account Permissions to: Create IAM Roles Create Lambda functions Create DynamoDB/S3 resources Role for API Gateway Typically, API Gateway doesn\u0026rsquo;t need a separate IAM role to authorize User Pool. However, for API Gateway to invoke Lambda from integration, you need to add permission to Lambda (resource-based) ‚Äî usually done via CLI or Console auto-create.\nRole for Lambda Functions Purpose: Lambda needs to write logs, access S3/DynamoDB, call Cognito Admin API (if backend manages users).\nCreating Lambda Execution Role Console Steps:\nIAM ‚Üí Roles ‚Üí Create role AWS service ‚Üí Lambda ‚Üí Next Attach managed policies: AWSLambdaBasicExecutionRole (CloudWatch logs) AmazonDynamoDBFullAccess (replace with restricted table-specific policy) AmazonS3ReadOnlyAccess or custom S3 policy DynamoDB Permissions Policy for Article CRUD operations:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/TravelGuide-*\u0026#34; } Why these permissions?\nLambda creates/edits/deletes articles Only access specific tables needed Follows least privilege principle S3 Permissions (Images) Policy for image upload/download:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::travel-guide-*/\\*\u0026#34; } Use cases:\nUpload user-submitted images Retrieve images for display Process images for thumbnails Rekognition Permissions Policy for image analysis:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rekognition:DetectLabels\u0026#34;, \u0026#34;rekognition:DetectModerationLabels\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } Use cases:\nDetect labels in travel photos Content moderation for inappropriate images Auto-tagging images CloudWatch Logs Permissions Policy for logging:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } Why needed?\nDebug Lambda functions Monitor application behavior Track errors and performance Complete Lambda Execution Role Combined policy example:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/TravelGuide-*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::travel-guide-*/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rekognition:DetectLabels\u0026#34;, \u0026#34;rekognition:DetectModerationLabels\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Trust Policy Who can assume this role?\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } This allows Lambda service to assume the role and use its permissions.\nBest Practices 1. Least Privilege Principle Only grant permissions actually needed Avoid using * in Resource ARNs when possible Use specific actions instead of * 2. Separate Roles by Function Different Lambda functions = different roles Article service role ‚â† Media service role Easier to audit and manage 3. Use Managed Policies When Appropriate AWSLambdaBasicExecutionRole for logging Custom policies for business logic Combine managed + custom policies 4. Regular Audits Review unused permissions Check for overly permissive policies Use IAM Access Analyzer 5. Tag Resources { \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;ArticleService\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;staging\u0026#34; } ] } Resource-Based Permissions API Gateway Invoke Lambda Lambda resource policy:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:CreateArticle\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:execute-api:*:*:*/*/POST/articles\u0026#34; } } } This allows API Gateway to invoke the Lambda function.\nKey Takeaways IAM Roles define what AWS services can do Policies specify exact permissions Trust Policies define who can assume roles Least Privilege is critical for security Separate roles for different services Regular audits prevent permission creep Resource-based policies for cross-service access Common Pitfalls ‚ùå Using Administrator Access\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ‚úÖ Use Specific Permissions\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/Articles\u0026#34; } ‚ùå Sharing Roles Across Services\nOne role for all Lambda functions ‚úÖ Separate Roles\nArticleServiceRole MediaServiceRole AIServiceRole "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/","title":"Infrastructure as Code - Multi-Stack Pattern","tags":[],"description":"","content":"Infrastructure as Code with Multi-Stack Deployment This section explains the Infrastructure as Code (IaC) strategy used to build the Travel Guide Application infrastructure from scratch using CloudFormation/SAM with a multi-stack deployment pattern.\nOverview The Travel Guide Application is built using a microservices architecture deployed across multiple AWS CloudFormation stacks. This approach provides better isolation, faster deployments, and reduced blast radius when making changes.\nKey Concepts Infrastructure as Code: All infrastructure defined in CloudFormation templates Multi-Stack Pattern: Separation of core resources and service-specific resources Cross-Stack References: Sharing resources between stacks using CloudFormation Exports/Imports Deployment Orchestration: Automated deployment using Bash scripts Environment Management: Parameter-based configuration for multiple environments Content IaC Strategy \u0026amp; Tool Selection Multi-Stack Architecture Cross-Stack References Deployment Orchestration Parameter Management Lessons Learned \u0026amp; Best Practices "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/5.4.2-detect-labels/","title":"Lambda Detect Labels","tags":[],"description":"","content":"Purpose Analyze images and automatically add descriptive labels such as:\nLocations (beach, mountain, city\u0026hellip;) Objects (people, food, building\u0026hellip;) Activities (swimming, hiking, dining\u0026hellip;) Main Code Explanation a. Receive message from SQS for sqs_record in event.get(\u0026#39;Records\u0026#39;, []): try: # Parse S3 event from SQS message body s3_event = json.loads(sqs_record[\u0026#39;body\u0026#39;]) # Process each S3 record in the event for s3_record in s3_event.get(\u0026#39;Records\u0026#39;, []): try: # Extract S3 information bucket = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] b. Call rekognition.detect_labels Detect and prioritize important labels:\n# Detect and prioritize labels labels_data = detect_labels_in_image(bucket, key) if not labels_data: print(\u0026#34;No labels detected after prioritization\u0026#34;) results[\u0026#39;failed\u0026#39;] += 1 continue c. Save labels to ArticlesTable and Gallery Update article with tags and save to Gallery:\n# Update article success = update_article_with_tags(article_id, labels_data) if success: results[\u0026#39;succeeded\u0026#39;] += 1 # Save to Gallery tables try: import sys import os # Add current directory to path current_dir = os.path.dirname(os.path.abspath(__file__)) if current_dir not in sys.path: sys.path.insert(0, current_dir) from save_to_gallery import save_photo_to_gallery, update_trending_tags tag_names = [label[\u0026#39;name\u0026#39;] for label in labels_data] image_url = key # S3 key photo_id = key # Use S3 key as unique identifier save_photo_to_gallery( photo_id, image_url, tag_names, status=\u0026#39;public\u0026#39;, article_id=article_id ) update_trending_tags(tag_names, image_url) print(\u0026#34;‚úì Saved to Gallery tables\u0026#34;) Demo: Data After Processing After successful processing, data is stored in DynamoDB tables:\nProcessing Flow Receive moderated image from SQS Call Rekognition detect_labels Filter and prioritize important labels Update ArticlesTable with tags Save metadata to GalleryPhotosTable Update statistics in GalleryTrendsTable Result Articles are automatically tagged Images appear in Gallery with tags Trending tags are updated Users can search by tags "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.2-multistack-architecture/","title":"Multi-Stack Architecture","tags":[],"description":"","content":"Multi-Stack Pattern Deep Dive What is Multi-Stack Pattern? Instead of deploying all infrastructure in a single CloudFormation stack, we separate resources into multiple stacks based on their lifecycle, dependencies, and blast radius.\nStack Separation Strategy Core Stack (Stateful Resources) Contains shared, stateful resources that rarely change:\nDynamoDB Tables: ArticlesTable, UserProfilesTable, GalleryPhotosTable, etc. S3 Buckets: Image storage, deployment artifacts Cognito User Pool: User authentication VPC \u0026amp; Networking: If needed IAM Roles: Shared execution roles Characteristics:\nChanges infrequently Shared across multiple services High cost of recreation Long deployment time (5-10 minutes) Service Stacks (Stateless Resources) Each microservice has its own stack:\nLambda Functions: Service-specific business logic API Gateway: Service endpoints EventBridge Rules: Service-specific events SQS Queues: Service-specific queues SNS Topics: Service-specific notifications Characteristics:\nChanges frequently Independent deployment Fast deployment (2-3 minutes) Low recreation cost Stack Separation Criteria Criteria Core Stack Service Stack Change Frequency Rarely (monthly) Often (daily/weekly) State Stateful (data) Stateless (compute) Sharing Shared across services Service-specific Deployment Time Long (5-10 min) Short (2-3 min) Blast Radius High (affects all) Low (affects one service) Recreation Cost High (data loss risk) Low (no data loss) Benefits of Multi-Stack Pattern 1. Blast Radius Reduction Problem: In a monolithic stack, any change affects all resources.\nSolution: With multi-stack, updating Article Service doesn\u0026rsquo;t impact Auth or Media services.\nExample:\n# Update only Article Service ./deploy-service.sh article-service staging # Other services (auth, media, gallery) remain untouched Impact: üî¥ High - Prevents cascading failures\n2. Independent Deployment Problem: Can\u0026rsquo;t deploy services independently in monolithic stack.\nSolution: Each service can be deployed, rolled back, or updated independently.\nExample:\n# Deploy new feature to Article Service ./deploy-service.sh article-service staging # Rollback if issues found aws cloudformation delete-stack --stack-name travel-guide-article-service-staging # Other services continue running normally Impact: üî¥ High - Enables continuous deployment\n3. Faster Deployment Problem: Monolithic stack takes 15-20 minutes to deploy.\nSolution: Service stacks deploy in 2-3 minutes.\nComparison:\nMonolithic Stack: 15-20 minutes (all resources) Core Stack: 5-10 minutes (one-time) Service Stack: 2-3 minutes (frequent) Impact: üü° Medium - Improves developer productivity\n4. Parallel Development Problem: Teams block each other when working on same stack.\nSolution: Teams can work on different service stacks simultaneously.\nExample:\nTeam A: Updates Article Service Team B: Updates Media Service Team C: Updates Gallery Service No conflicts, no waiting Impact: üî¥ High - Enables team scalability\n5. Easier Rollback Problem: Rolling back monolithic stack affects all services.\nSolution: Rollback only the affected service stack.\nExample:\n# Bug found in Article Service aws cloudformation delete-stack --stack-name travel-guide-article-service-staging # Redeploy previous version git checkout v1.2.3 ./deploy-service.sh article-service staging # Auth, Media, Gallery services unaffected Impact: üî¥ High - Reduces downtime\n6. Resource Limit Management Problem: CloudFormation has 500 resource limit per stack.\nSolution: Distribute resources across multiple stacks.\nExample:\nCore Stack: ~50 resources (tables, buckets, pools) Each Service Stack: ~20 resources (lambdas, APIs) Total: 6 services √ó 20 = 120 resources (well under limit) Impact: üü¢ Low - Prevents hitting limits\n7. Cost Optimization Problem: Can\u0026rsquo;t optimize costs per service in monolithic stack.\nSolution: Tag and track costs per service stack.\nExample:\nTags: - Key: Service Value: ArticleService - Key: Environment Value: staging - Key: CostCenter Value: Engineering Impact: üü° Medium - Enables cost attribution\nComparison: Monolithic vs Multi-Stack Aspect Monolithic Stack Multi-Stack Pattern Deployment Time 15-20 minutes 2-3 minutes (per service) Blast Radius All services affected Single service affected Rollback All services rolled back Single service rolled back Team Collaboration Sequential (blocking) Parallel (non-blocking) Resource Limit 500 (can hit limit) 500 per stack (scalable) Cost Tracking Difficult Easy (per service) Complexity Low (single stack) Medium (multiple stacks) Maintenance Difficult (large template) Easier (small templates) Stack Dependencies graph TD Core[Core Stack] --\u0026gt; Auth[Auth Service] Core --\u0026gt; Article[Article Service] Core --\u0026gt; Media[Media Service] Core --\u0026gt; AI[AI Service] Core --\u0026gt; Gallery[Gallery Service] Core --\u0026gt; Notification[Notification Service] style Core fill:#ff9999 style Auth fill:#99ccff style Article fill:#99ccff style Media fill:#99ccff style AI fill:#99ccff style Gallery fill:#99ccff style Notification fill:#99ccff Deployment Order:\nDeploy Core Stack first (provides shared resources) Deploy Service Stacks in any order (parallel possible) Key Takeaways Separate stateful and stateless resources into different stacks Core Stack contains shared, rarely-changing resources Service Stacks contain service-specific, frequently-changing resources Benefits include faster deployments, reduced blast radius, and better team collaboration Trade-off is increased complexity in managing multiple stacks Cross-stack references enable resource sharing between stacks When to Use Multi-Stack ‚úÖ Use Multi-Stack when:\nBuilding microservices architecture Multiple teams working on different services Frequent deployments required Need independent service lifecycle ‚ùå Avoid Multi-Stack when:\nSimple monolithic application Single team, infrequent deployments All resources tightly coupled Overhead not justified "},{"uri":"https://tqunhh.github.io/FCJ-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":" üìÑ Download Full Proposal: Proposal Template.docx\nTravel Journal A Unified AWS Serverless Solution for Travel Journal 1. Executive Summary Travel Journal Web was created to help people preserve their life journeys ‚Äî not only the trips they take but also the memories, emotions, and stories behind each photo. The application connects people to their experiences, turning every journey into part of their own ‚Äúmemory map.‚Äù\nUsers can upload photos, add location notes, and the system automatically recognizes the scene type (beach, mountain, city, etc.) using Amazon Rekognition. Travel routes are displayed visually and in real-time on an interactive map powered by Amazon Location Service, delivering a vivid and engaging experience.\nThe platform leverages the power of AWS Serverless Architecture ‚Äî including Lambda, API Gateway, S3, DynamoDB, and Cognito ‚Äî ensuring high performance, strong security, and scalable flexibility at an optimized cost.\n2. Problem Statement What‚Äôs the Problem? Many travel enthusiasts want to document their journeys, yet existing platforms only allow posting scattered images or notes without an intuitive connection between emotions, photos, and actual locations. It becomes difficult to organize memories, view trips on a map, or analyze travel data such as destinations, durations, and experiences. Moreover, popular cloud storage solutions fail to personalize the user experience.\nThe Solution Travel Journal Web is built as a web application leveraging the AWS Serverless architecture to optimize performance and cost efficiency.\nThe system uses Amazon S3 to store images and static data, distributed globally through Amazon CloudFront. Amazon Cognito manages secure user authentication, while API Gateway and AWS Lambda handle backend logic. Uploaded images in S3 are analyzed by Amazon Rekognition, with results and location data stored in Amazon DynamoDB and visualized using Amazon Location Service.\nThe entire system is monitored by Amazon CloudWatch, tracking throughput, errors, latency, and database capacity; alerts are sent via SNS. Data and encryption keys are securely managed using AWS KMS and Secrets Manager.\nThis solution delivers a smart, secure, and cost-effective travel application, allowing users to easily capture, store, and revisit their journeys anytime, anywhere..\nBenefits and Return on Investment The solution allows users to easily record and share their journeys while establishing a data foundation for potential expansion into a social travel platform. With an estimated cost of only USD 14.55/month, the app can support 100‚Äì200 users without physical servers. The expected ROI period is 6‚Äì8 months, achieved by saving maintenance and storage costs.\n3. Solution Architecture The Travel Journal Web is built entirely on an AWS Serverless architecture, optimizing performance, security, and scalability. The static web interface is hosted on Amazon S3, distributed globally through Amazon CloudFront, and protected by AWS WAF, ACM, and Route 53. User authentication is managed by Amazon Cognito, while Amazon API Gateway and AWS Lambda handle backend business logic. Uploaded images are stored in Amazon S3 and automatically processed through Amazon SQS, AWS Lambda, Amazon Rekognition, and Amazon Location Service. The results are stored in Amazon DynamoDB and redistributed via S3. The system supports retry and DLQ mechanisms, sends notifications through Amazon SNS, provides centralized monitoring with Amazon CloudWatch and AWS X-Ray, and secures data using AWS IAM, KMS, and Secrets Manager.. The architecture is detailed below:\nAWS Services Used Amazon Route 53: Global domain management and routing. AWS Certificate Manager (ACM): Issues and manages SSL/TLS certificates for secure endpoints Amazon CloudFront: Low-latency content delivery for static and dynamic assets. AWS WAF: Protects the application from common web threats. AWS Lambda: Event-driven serverless compute for backend logic. Amazon API Gateway: Acts as the interface between the frontend and Lambda backend. Amazon S3: Stores user data, images, and activity logs Amazon DynamoDB: NoSQL database for trip records and user data, optimized for fast queries. Amazon Cognito: User authentication and authorization management. Amazon Rekognition: Image analysis and labeling. Amazon Location Service: Geolocation and map visualization. Amazon SNS: Sends notifications to users and administrators. Amazon SQS (Main Queue): Buffers image processing requests from S3 uploads before invoking Lambda. Dead Letter Queue (DLQ): Captures failed or unprocessed messages from SQS for later review and troubleshooting. Amazon CloudWatch: Logs, metrics, and performance monitoring. AWS IAM: Manages access roles and permissions for AWS services. AWS KMS: Encrypts data at rest and in transit, enhancing overall security. AWS Secrets Manager: Securely stores and encrypts confidential credentials. AWS CodeBuild: Automatically compiles, tests, and packages source code. AWS CodePipeline: Automates the entire CI/CD process ‚Äî from commit, build, and test to deployment in AWS environments. Component Design User Authentication: Managed by Amazon Cognito for login, token management, and access control. Application Logic: AWS Lambda handles travel data submissions, image uploads, and analytics requests from API Gateway. Data Management: Amazon DynamoDB stores journey details; OpenSearch provides fast indexing and search. Queue Processing: Amazon SQS (Main Queue) receives image processing requests from S3 before invoking Lambda; the Dead Letter Queue (DLQ) captures failed messages for later handling. Image Analysis: Amazon Rekognition automatically labels and classifies photo content. Maps \u0026amp; Location Data: Amazon Location Service tracks and displays GPS data on interactive maps. Content Storage \u0026amp; Delivery: Amazon S3 stores images, user data, and static assets; content is distributed globally via Amazon CloudFront (protected by AWS WAF, SSL/TLS via ACM, and routed through Route 53). Monitoring \u0026amp; Notifications: Amazon CloudWatch monitors logs and performance metrics; Amazon SNS sends alerts and notifications to users. Security \u0026amp; Access Management: AWS IAM manages service permissions, while AWS Secrets Manager and KMS protect sensitive information. Deployment \u0026amp; CI/CD: AWS CodeBuild and CodePipeline automate the build, test, and deployment processes. 4. Technical Implementation Implementation Phases The project is divided into two main areas ‚Äî web application development and AWS infrastructure integration ‚Äî across three stages:\nRequirement Analysis \u0026amp; Architecture Design: Identify suitable AWS services (CloudFront, WAF, Cognito, DynamoDB, Lambda, API Gateway, S3, etc.) and design the overall architecture (Month 1). Cost Estimation \u0026amp; System Simulation: Calculate costs using AWS Pricing Calculator, test image and location processing workflows, and optimize system resources (Month 2). Development, Testing \u0026amp; Deployment: Build the web application, integrate AWS SDKs, test WAF protection, and monitor operations using CloudWatch (Month 3). Technical Requirements\nFrontend: Built with React.js, deployed as static files on Amazon S3, and globally distributed via CloudFront for fast load times and reduced backend load. Security \u0026amp; Access: AWS WAF protects against common web attacks; AWS Certificate Manager (ACM) provides SSL/TLS certificates; Amazon Cognito manages user authentication (email, OAuth2). Backend: API Gateway routes user requests to AWS Lambda, which executes business logic such as trip logging, image uploads, and data queries. Queue Processing: Amazon SQS (Main Queue) buffers image processing requests triggered from S3, while the Dead Letter Queue (DLQ) stores failed messages for later handling. Database \u0026amp; Search: Amazon DynamoDB stores user journeys and notes Image Analysis: Amazon Rekognition detects scenes, faces, and automatically suggests image labels. Geolocation \u0026amp; Maps: Amazon Location Service visualizes GPS coordinates on an interactive map. Monitoring \u0026amp; Security: Amazon CloudWatch collects logs; AWS Secrets Manager and KMS secure sensitive information Notifications: Amazon SNS sends alerts for system errors or new user events. Deployment \u0026amp; CI/CD: AWS CodeBuild and AWS CodePipeline automate the build, test, and deployment workflow. 5. Timeline \u0026amp; Milestones Project Timeline\nInternship (Months 1-3): 3 months. Month 1: Learn AWS fundamentals. Month 2: Design architecture, estimate costs, and refine infrastructure. Month 3: Implement, test, and deploy the application. Post-Launch:Continue system research and improvements over the following year. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator\nInfrastructure Costs AWS Services: Amazon Route 53: 0,50 USD/month AWS Certificate Manager: 0,0 USD/month Amazon CloudFront: 0,61 USD/month AWS WAF: 0,6 USD/month AWS Lambda: 0,01 USD/month Amazon API Gateway: 0,45 USD/month Amazon S3: 1,47 USD/month Amazon DynamoDB: 16,35 USD/month Amazon Cognito: 5,00 USD/month Amazon Rekognition: 10,08 USD/month Amazon Location Service: 4,35 USD/month Amazon SNS: 2,58 USD/month Amazon SQS (Main Queue): 3,1 USD/month Amazon CloudWatch: 6,87 USD/month AWS IAM: 0,00 USD/month AWS Secrets Manager: 0,4 USD/month AWS KMS: 2,3 USD/month AWS CodeBuild: 0,8 USD/month AWS CodePipeline: 0,00 USD/month Total: 61.78 USD/month, or $946,56 USD/year\n7. Risk Assessment Risk Matrix Security breach or user access loss: High impact, very low probability. Cost increase due to Rekognition usage: High impact, medium probability. GPS data inconsistency: High impact, low probability Mitigation Strategies Network: Use CloudFront for global caching and stable performance across regions. Infrastructure: Leverage Lambda‚Äôs automatic retry mechanism and browser caching to minimize downtime. Security: Apply multi-layer authentication via Cognito and strict IAM/S3 permissions. Cost: Set budget alerts in AWS Budgets, and optimize Lambda/S3 usage based on access patterns. Contingency Plans Integrate SNS and CloudWatch Alerts to notify admins immediately of failures (e.g., Lambda errors, API overloads, budget overruns). Use CloudFormation for rapid infrastructure recovery Enable S3 Versioning to preserve image and log backups. 8. Expected Outcomes Technical Improvements: The app automates storage, analysis, and visualization of travel data on a map, eliminating manual note-taking.\nLong-term Value Builds a rich dataset of travel routes, photos, and emotions ‚Äî forming a foundation for future applications like personalized travel recommendations and experience analytics.\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives: Attend AWS Cloud Mastery Series #1 event Write documentary proposal Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Design post display frame on Homepage Complete functions used on page - Display thumbnail on map, display correct location that user attached to post 10/11/2025 10/11/2025 3 - Backend CORS error, support Backend to fix error - Complete using buttons, navigate successfully 11/11/2025 11/11/2025 4 - Write documentary proposal for project Fix some user design errors 12/11/2025 12/11/2025 5 - Combine with Backend to check API Endpoint - Successfully display posts on interface 13/11/2025 13/11/2025 6 - Deploy and test post deletion function - Backend changes deployment environment, fix some errors for completion - Continue completing functions with minor errors 14/11/2025 14/11/2025 7 - Attend AWS Cloud Mastery Series #1 event 15/11/2025 15/11/2025 Week 10 Achievements: Complete Homepage interface:\nDesign post display frame Complete functions used on page Display post thumbnail on map, correct location user attached Coordinate with Backend to handle CORS error, ensure stable API calls\nComplete navigation, function buttons work correctly\nWrite project documentary proposal\nCombine with Backend to test API Endpoint, successfully display posts on interface\nDeploy and test post deletion function\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Fix post update errors Attend AWS Cloud Mastery Series #2 event Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Attend AWS Cloud Mastery Series #2 event 17/11/2025 17/11/2025 3 - Unify and restructure Frontend: Conduct team meeting to unify Frontend code structure to ensure consistency 18/11/2025 18/11/2025 4 - Design Christmas effects - Fix post update errors - Complete workshop assignment 19/11/2025 19/11/2025 5 - Synchronize language for website - Add Engmode to give users language choice 20/11/2025 20/11/2025 6 - Team meeting to check new Frontend structure, stabilize main project Stack and synchronize CORS fixes and Template. - Add function to display coordinates when selecting location 21/11/2025 21/11/2025 Week 11 Achievements: Attend AWS Cloud Mastery Series #2 event\nUnify and restructure Frontend\nDesign Christmas effects\nSynchronize\nlanguage for website CORS fixes and Template. Complete workshop assignment\n"},{"uri":"https://tqunhh.github.io/FCJ-report/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Test Rekognition AI function with Backend team Attend AWS Cloud Mastery Series #3 event Tasks to be implemented this week: Day Tasks Start Date Completion Date Resources 2 - Test Rekognition AI function with Backend team - Find solutions to fix errors - Recognize objects and scenes in images 24/11/2025 24/11/2025 3 - After Backend update, discovered more Frontend errors, team meeting to fix errors urgently - Complete fixing existing errors 25/11/2025 25/11/2025 4 - Team meeting to design presentation slides for project 26/11/2025 26/11/2025 5 - Combine with Backend team, complete notification function: + Successfully post + Successfully update + Delete/edit post 27/11/2025 27/11/2025 6 - Overall check of web functions - Test deployed web - Update data - Fix avatar update function, cover image 28/11/2025 28/11/2025 7 - Attend AWS Cloud Mastery Series #3 event 29/11/2025 29/11/2025 Week 12 Achievements: Test Rekognition AI function\nTeam meeting to design presentation slides\nCombine with Backend team, complete notification function\nOverall check of web functions\n"},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.6-cloudfront-s3-location/5.6.3-location-service/","title":"AWS Location Service Integration","tags":[],"description":"","content":"AWS Location Service (Map/Geocoding) Components Used in the Project The project uses Place Index for geocoding, without requiring Map tiles.\nPlace Index: TravelGuidePlaceIndex (Esri)\nMain APIs:\nSearchPlaceIndexForPosition (reverse geocoding) SearchPlaceIndexForText (forward geocoding) Place Index Configuration Creating Place Index AWS Console Steps:\nNavigate to Amazon Location Service Create Place Index Configure: Name: TravelGuidePlaceIndex Data provider: Esri Pricing plan: RequestBasedUsage Intended use: SingleUse SAM Template:\nTravelGuidePlaceIndex: Type: AWS::Location::PlaceIndex Properties: IndexName: TravelGuidePlaceIndex DataSource: Esri PricingPlan: RequestBasedUsage DataSourceConfiguration: IntendedUse: SingleUse Backend Integration How Backend Calls Location Service In functions/utils/location_service.py:\nLogic flow:\nCheck DynamoDB cache If cache miss ‚Üí call AWS Location If error/no results ‚Üí fallback to Nominatim Save to cache with TTL (default 30 days) Lambda Functions Using Location Service Functions:\nCreateArticleFunction UpdateArticleFunction Environment Variables:\nEnvironment: Variables: PLACE_INDEX_NAME: !Ref TravelGuidePlaceIndex LOCATION_CACHE_TABLE: !Ref LocationCacheTable USE_AWS_LOCATION: \u0026#39;true\u0026#39; Python Code Example import boto3 import json from datetime import datetime, timedelta location_client = boto3.client(\u0026#39;location\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) cache_table = dynamodb.Table(os.environ[\u0026#39;LOCATION_CACHE_TABLE\u0026#39;]) PLACE_INDEX = os.environ[\u0026#39;PLACE_INDEX_NAME\u0026#39;] def reverse_geocode(latitude, longitude): \u0026#34;\u0026#34;\u0026#34;Convert coordinates to place name\u0026#34;\u0026#34;\u0026#34; # Generate cache key cache_key = f\u0026#34;reverse:{latitude:.6f},{longitude:.6f}\u0026#34; # Check cache try: response = cache_table.get_item(Key={\u0026#39;cacheKey\u0026#39;: cache_key}) if \u0026#39;Item\u0026#39; in response: cached_data = response[\u0026#39;Item\u0026#39;] # Check if not expired if datetime.now().timestamp() \u0026lt; cached_data[\u0026#39;expiresAt\u0026#39;]: print(f\u0026#34;Cache HIT: {cache_key}\u0026#34;) return json.loads(cached_data[\u0026#39;data\u0026#39;]) except Exception as e: print(f\u0026#34;Cache read error: {e}\u0026#34;) print(f\u0026#34;Cache MISS: {cache_key}\u0026#34;) # Call AWS Location Service try: response = location_client.search_place_index_for_position( IndexName=PLACE_INDEX, Position=[longitude, latitude], # Note: [lng, lat] MaxResults=1 ) if response[\u0026#39;Results\u0026#39;]: place = response[\u0026#39;Results\u0026#39;][0][\u0026#39;Place\u0026#39;] result = { \u0026#39;label\u0026#39;: place.get(\u0026#39;Label\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;municipality\u0026#39;: place.get(\u0026#39;Municipality\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;country\u0026#39;: place.get(\u0026#39;Country\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;region\u0026#39;: place.get(\u0026#39;Region\u0026#39;, \u0026#39;\u0026#39;) } # Save to cache save_to_cache(cache_key, result) return result except Exception as e: print(f\u0026#34;AWS Location error: {e}\u0026#34;) # Fallback to Nominatim return fallback_nominatim(latitude, longitude) return None def forward_geocode(text): \u0026#34;\u0026#34;\u0026#34;Convert place name to coordinates\u0026#34;\u0026#34;\u0026#34; cache_key = f\u0026#34;forward:{text}\u0026#34; # Check cache (similar to reverse) # ... # Call AWS Location Service try: response = location_client.search_place_index_for_text( IndexName=PLACE_INDEX, Text=text, MaxResults=5 ) results = [] for item in response[\u0026#39;Results\u0026#39;]: place = item[\u0026#39;Place\u0026#39;] results.append({ \u0026#39;label\u0026#39;: place.get(\u0026#39;Label\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;position\u0026#39;: place[\u0026#39;Geometry\u0026#39;][\u0026#39;Point\u0026#39;], # [lng, lat] \u0026#39;country\u0026#39;: place.get(\u0026#39;Country\u0026#39;, \u0026#39;\u0026#39;) }) # Save to cache save_to_cache(cache_key, results) return results except Exception as e: print(f\u0026#34;AWS Location error: {e}\u0026#34;) return [] def save_to_cache(key, data, ttl_days=30): \u0026#34;\u0026#34;\u0026#34;Save geocoding result to DynamoDB cache\u0026#34;\u0026#34;\u0026#34; try: expires_at = int((datetime.now() + timedelta(days=ttl_days)).timestamp()) cache_table.put_item( Item={ \u0026#39;cacheKey\u0026#39;: key, \u0026#39;data\u0026#39;: json.dumps(data), \u0026#39;expiresAt\u0026#39;: expires_at, \u0026#39;createdAt\u0026#39;: int(datetime.now().timestamp()) } ) print(f\u0026#34;Saved to cache: {key}\u0026#34;) except Exception as e: print(f\u0026#34;Cache write error: {e}\u0026#34;) def fallback_nominatim(latitude, longitude): \u0026#34;\u0026#34;\u0026#34;Fallback to Nominatim if AWS Location fails\u0026#34;\u0026#34;\u0026#34; import requests try: url = f\u0026#34;https://nominatim.openstreetmap.org/reverse\u0026#34; params = { \u0026#39;lat\u0026#39;: latitude, \u0026#39;lon\u0026#39;: longitude, \u0026#39;format\u0026#39;: \u0026#39;json\u0026#39; } headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;TravelGuideApp/1.0\u0026#39;} response = requests.get(url, params=params, headers=headers, timeout=5) if response.status_code == 200: data = response.json() return { \u0026#39;label\u0026#39;: data.get(\u0026#39;display_name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;municipality\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;city\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;country\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;country\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;region\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;state\u0026#39;, \u0026#39;\u0026#39;) } except Exception as e: print(f\u0026#34;Nominatim fallback error: {e}\u0026#34;) return None DynamoDB Cache for Cost Optimization LocationCacheTable Structure Table Schema:\nLocationCacheTable: Type: AWS::DynamoDB::Table Properties: TableName: LocationCache BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: cacheKey AttributeType: S KeySchema: - AttributeName: cacheKey KeyType: HASH TimeToLiveSpecification: Enabled: true AttributeName: expiresAt Attributes:\ncacheKey (String, Primary Key): reverse:lat,lng or forward:text data (String): JSON-encoded geocoding result expiresAt (Number): Unix timestamp for TTL createdAt (Number): Creation timestamp Benefits:\nReduces AWS Location Service requests Saves costs (Location Service is request-based) Faster response times Automatic cleanup via TTL IAM Permissions Lambda Execution Role Minimum IAM policy for Lambda:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:geo:ap-southeast-1:*:place-index/TravelGuidePlaceIndex\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/LocationCache\u0026#34; } ] } SAM Policy Templates:\nPolicies: - DynamoDBCrudPolicy: TableName: !Ref LocationCacheTable - Statement: - Effect: Allow Action: - geo:SearchPlaceIndexForPosition - geo:SearchPlaceIndexForText Resource: !GetAtt TravelGuidePlaceIndex.Arn Operations \u0026amp; Cost Optimization Monitoring Cache Performance Metrics to track:\nCache hit ratio Cache miss ratio AWS Location Service request count Average response time CloudWatch Custom Metrics:\nimport boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def log_cache_metric(metric_name, value): cloudwatch.put_metric_data( Namespace=\u0026#39;TravelGuide/Location\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: value, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Usage log_cache_metric(\u0026#39;CacheHit\u0026#39;, 1) log_cache_metric(\u0026#39;CacheMiss\u0026#39;, 1) log_cache_metric(\u0026#39;LocationServiceCall\u0026#39;, 1) Cost Optimization Strategies Increase Cache TTL\nFor stable locations: 90 days For changing data: 7-30 days Cache Popular Queries\nPre-populate cache for common locations Reduce cold start costs Batch Requests (if applicable)\nGroup multiple geocoding requests Reduce API calls Monitor Usage\nSet CloudWatch alarms for high request counts Review monthly Location Service costs Fallback Strategy\nUse Nominatim for non-critical requests Reserve AWS Location for premium features Frontend Integration (Optional) Using AWS Location Maps If you want to use AWS Location Maps (instead of OSM/Esri public tiles), you have two approaches:\n1. API Key (Amazon Location API Key) Easy integration Suitable for public web Has quota/limits 2. Cognito + SigV4 Signed Requests More secure More complex setup Better for authenticated users Leaflet Integration Example import L from \u0026#39;leaflet\u0026#39;; import { Signer } from \u0026#39;@aws-amplify/core\u0026#39;; // With API Key const map = L.map(\u0026#39;map\u0026#39;).setView([10.8231, 106.6297], 13); L.tileLayer( \u0026#39;https://maps.geo.ap-southeast-1.amazonaws.com/maps/v0/maps/{mapName}/tiles/{z}/{x}/{y}?key={apiKey}\u0026#39;, { attribution: \u0026#39;¬© Amazon Location Service\u0026#39;, mapName: \u0026#39;TravelGuideMap\u0026#39;, apiKey: \u0026#39;YOUR_API_KEY\u0026#39; } ).addTo(map); // With Cognito (SigV4) // More complex - requires signing each tile request // See AWS documentation for full implementation Troubleshooting Common Issues 1. No Results from Location Service Causes:\nInvalid coordinates Location not in Esri database Incorrect Place Index configuration Solution:\nValidate lat/lng ranges (-90 to 90, -180 to 180) Use fallback to Nominatim Check Place Index data source 2. High Costs Causes:\nLow cache hit ratio Too many unique queries Short cache TTL Solution:\nIncrease cache TTL Pre-populate common locations Review query patterns 3. Slow Response Times Causes:\nCold Lambda start Cache miss Network latency Solution:\nUse provisioned concurrency for Lambda Pre-warm cache Implement async geocoding for non-critical paths 4. Cache Not Working Causes:\nDynamoDB permissions missing TTL not configured Cache key mismatch Solution:\nVerify IAM permissions Check DynamoDB TTL settings Debug cache key generation Best Practices Always Use Cache\nReduces costs significantly Improves performance Implement Fallback\nNominatim for reliability Handle service outages gracefully Validate Input\nCheck coordinate ranges Sanitize text queries Monitor Usage\nTrack request counts Set cost alerts Optimize TTL\nLonger for stable data Shorter for dynamic data Log Cache Performance\nHit/miss ratios Identify optimization opportunities Key Takeaways AWS Location Service provides geocoding without managing infrastructure DynamoDB cache dramatically reduces costs Fallback strategy ensures reliability Proper IAM permissions are essential Monitoring helps optimize performance and costs Cache TTL balances freshness and cost "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.7-security/5.7.3-s3-ownership-validation/","title":"S3 Ownership Validation","tags":[],"description":"","content":"S3 Ownership Validation - Prevent Unauthorized Access The Problem Before implementation:\nUsers could reference ANY S3 object in their articles No ownership validation Users could use other users\u0026rsquo; images Potential data leakage and storage abuse Attack scenario:\n1. User A uploads image: articles/user-a-id/photo.jpg 2. User B creates article with User A\u0026#39;s image key 3. User B\u0026#39;s article displays User A\u0026#39;s private photo 4. ‚ùå Unauthorized access! The Solution Implement ownership validation to ensure users can only use their own images.\nValidation checks:\nImage key format validation Article ID matching File size limits File type validation Ownership verification Implementation Updated: create_article.py Added validation logic:\nfrom security_utils import validate_image_key import boto3 s3_client = boto3.client(\u0026#39;s3\u0026#39;) BUCKET_NAME = os.environ[\u0026#39;ARTICLE_IMAGES_BUCKET\u0026#39;] def lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] article_id = str(uuid.uuid4()) # Get image keys from request image_keys = data.get(\u0026#34;imageKeys\u0026#34;, []) # ‚úÖ Validate each image for key_str in image_keys: try: # 1. Validate key format and ownership validate_image_key(key_str, article_id, user_id) # 2. Check if image exists response = s3_client.head_object( Bucket=BUCKET_NAME, Key=key_str ) # 3. Validate file size file_size = response[\u0026#39;ContentLength\u0026#39;] if file_size \u0026gt; 10 * 1024 * 1024: # 10 MB return error(400, f\u0026#34;Image {key_str} is too large (max 10MB)\u0026#34;) # 4. Validate file type content_type = response.get(\u0026#39;ContentType\u0026#39;, \u0026#39;\u0026#39;) allowed_types = [\u0026#39;image/jpeg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/gif\u0026#39;, \u0026#39;image/webp\u0026#39;] if content_type not in allowed_types: return error(400, f\u0026#34;Invalid image type: {content_type}\u0026#34;) except s3_client.exceptions.NoSuchKey: return error(404, f\u0026#34;Image not found: {key_str}\u0026#34;) except PermissionError as e: return error(403, str(e)) except Exception as e: return error(500, f\u0026#34;Error validating image: {str(e)}\u0026#34;) # All images validated - proceed with article creation # ... Validation Functions 1. Key Format Validation Function: validate_image_key(key, article_id, owner_id)\ndef validate_image_key(key: str, article_id: str, owner_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Validate that image key belongs to the article and user Expected format: articles/{article_id}/raw/{filename} or: articles/{article_id}/thumbnails/{filename} \u0026#34;\u0026#34;\u0026#34; # Basic S3 key validation validate_s3_key(key) # Check if key starts with correct article path expected_prefix = f\u0026#34;articles/{article_id}/\u0026#34; if not key.startswith(expected_prefix): raise PermissionError( f\u0026#34;Image does not belong to this article. \u0026#34; f\u0026#34;Expected prefix: {expected_prefix}\u0026#34; ) # Additional checks for path traversal if \u0026#39;..\u0026#39; in key or \u0026#39;//\u0026#39; in key: raise PermissionError(\u0026#34;Invalid image path detected\u0026#34;) return True 2. File Size Validation def validate_file_size(file_size: int, max_size: int = 10 * 1024 * 1024) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate file size (default max: 10 MB)\u0026#34;\u0026#34;\u0026#34; if file_size \u0026gt; max_size: raise ValueError(f\u0026#34;File too large: {file_size} bytes (max: {max_size})\u0026#34;) if file_size == 0: raise ValueError(\u0026#34;File is empty\u0026#34;) return True 3. File Type Validation def validate_file_type(content_type: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate file MIME type\u0026#34;\u0026#34;\u0026#34; allowed_types = [ \u0026#39;image/jpeg\u0026#39;, \u0026#39;image/jpg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/gif\u0026#39;, \u0026#39;image/webp\u0026#39; ] if content_type not in allowed_types: raise ValueError( f\u0026#34;Invalid file type: {content_type}. \u0026#34; f\u0026#34;Allowed: {\u0026#39;, \u0026#39;.join(allowed_types)}\u0026#34; ) return True S3 Object Metadata Storing Ownership Information When uploading images, store metadata:\ndef upload_image_with_metadata(file, article_id, user_id): \u0026#34;\u0026#34;\u0026#34;Upload image with ownership metadata\u0026#34;\u0026#34;\u0026#34; s3_client.put_object( Bucket=BUCKET_NAME, Key=f\u0026#34;articles/{article_id}/raw/{filename}\u0026#34;, Body=file, ContentType=content_type, Metadata={ \u0026#39;article-id\u0026#39;: article_id, \u0026#39;owner-id\u0026#39;: user_id, \u0026#39;uploaded-at\u0026#39;: datetime.now().isoformat() } ) Verifying Metadata def verify_image_ownership(key: str, user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Verify image ownership using S3 metadata\u0026#34;\u0026#34;\u0026#34; try: response = s3_client.head_object( Bucket=BUCKET_NAME, Key=key ) metadata = response.get(\u0026#39;Metadata\u0026#39;, {}) owner = metadata.get(\u0026#39;owner-id\u0026#39;) if owner and owner != user_id: raise PermissionError(\u0026#34;You don\u0026#39;t own this image\u0026#34;) return True except s3_client.exceptions.NoSuchKey: raise FileNotFoundError(f\u0026#34;Image not found: {key}\u0026#34;) Testing Test 1: Valid Image (Own Article) # Upload image first curl -X POST https://api.example.com/upload-url \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; # Response: { \u0026#34;uploadUrl\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;articles/abc123/raw/image.jpg\u0026#34;, \u0026#34;articleId\u0026#34;: \u0026#34;abc123\u0026#34; } # Upload image to S3 curl -X PUT \u0026#34;$uploadUrl\u0026#34; \\ --upload-file image.jpg # Create article with own image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;My Article\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/image.jpg\u0026#34;] }\u0026#39; # Expected: 200 OK - Article created Test 2: Invalid Image (Other User\u0026rsquo;s Image) # Try to use another user\u0026#39;s image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Stolen Image\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/other-user-id/raw/image.jpg\u0026#34;] }\u0026#39; # Expected: 403 Forbidden # Error: \u0026#34;Image does not belong to this article\u0026#34; Test 3: File Too Large # Try to upload 20MB image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Large Image\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/large-image.jpg\u0026#34;] }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Image is too large (max 10MB)\u0026#34; Test 4: Invalid File Type # Try to use PDF file curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;PDF File\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/document.pdf\u0026#34;] }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Invalid image type: application/pdf\u0026#34; Security Benefits 1. Prevent Unauthorized Access ‚úÖ Before: Users could reference any S3 object ‚ùå After: Users can only use their own images\n2. Prevent Data Leakage ‚úÖ Before: Private images could be exposed ‚ùå After: Ownership validation prevents leakage\n3. Prevent Storage Abuse ‚úÖ Before: Users could link to unlimited images ‚ùå After: File size limits prevent abuse\n4. Prevent Malware Uploads ‚úÖ Before: Any file type accepted ‚ùå After: Only image types allowed\nBest Practices 1. Validate at Multiple Layers ‚úÖ Do: Validate in Lambda AND S3 bucket policy\nDefense in depth Multiple checkpoints 2. Use S3 Metadata ‚úÖ Do: Store ownership in metadata\nEasy to verify Immutable after upload 3. Implement Rate Limiting ‚úÖ Do: Limit upload frequency\nPrevent abuse Reduce costs # Example rate limiting from datetime import datetime, timedelta def check_rate_limit(user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if user exceeded upload rate limit\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;rate_limit:upload:{user_id}\u0026#34; # Get current count from cache count = cache.get(key) or 0 # Limit: 10 uploads per hour if count \u0026gt;= 10: raise PermissionError(\u0026#34;Upload rate limit exceeded\u0026#34;) # Increment counter cache.set(key, count + 1, ex=3600) # 1 hour TTL return True 4. Log All Validation Failures ‚úÖ Do: Log suspicious activity\nTrack attack attempts Identify patterns Alert on anomalies import logging logger = logging.getLogger() def log_validation_failure(user_id: str, key: str, reason: str): \u0026#34;\u0026#34;\u0026#34;Log validation failure for security monitoring\u0026#34;\u0026#34;\u0026#34; logger.warning( f\u0026#34;Validation failed - User: {user_id}, Key: {key}, Reason: {reason}\u0026#34;, extra={ \u0026#39;user_id\u0026#39;: user_id, \u0026#39;image_key\u0026#39;: key, \u0026#39;failure_reason\u0026#39;: reason, \u0026#39;timestamp\u0026#39;: datetime.now().isoformat() } ) Monitoring CloudWatch Metrics Track:\nValidation failures per user Invalid image attempts File size violations File type violations Create alarms:\n# Alert on high validation failure rate aws cloudwatch put-metric-alarm \\ --alarm-name high-validation-failures \\ --metric-name ValidationFailures \\ --namespace TravelGuide/Security \\ --statistic Sum \\ --period 300 \\ --threshold 10 \\ --comparison-operator GreaterThanThreshold CloudTrail Logging Monitor S3 access:\nTrack who accessed what Identify unauthorized attempts Audit trail for compliance Cost Impact Storage Costs Before:\nUsers could link unlimited images No size limits Potential abuse After:\n10 MB per image limit Controlled storage growth Predictable costs API Costs Additional S3 API calls:\nhead_object per image validation ~$0.0004 per 1,000 requests Negligible cost increase Key Takeaways Ownership validation prevents unauthorized image access File size limits prevent storage abuse File type validation prevents malware uploads S3 metadata stores ownership information Multiple validation layers provide defense in depth Logging and monitoring detect attack attempts "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.3-backend-articles/","title":"Backend - Article Service","tags":[],"description":"","content":"Backend Architecture Overview This section covers the complete backend implementation for the Article Management System. The backend is built using AWS serverless services including Lambda, DynamoDB, S3, Cognito, and AWS Location Service.\nThe architecture supports:\nFull CRUD operations for articles Image upload and management User favorites and interactions Gallery and trending features Location-based services with caching User profile management Content Lambda Service Backend DynamoDB Tables "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.3-cross-stack-references/","title":"Cross-Stack References","tags":[],"description":"","content":"Cross-Stack References Implementation Overview Cross-stack references allow one CloudFormation stack to use resources created by another stack. This is essential for the multi-stack pattern where service stacks need to reference resources from the core stack.\nCloudFormation Exports Mechanism The exporting stack (Core Stack) exposes resources using Outputs with Export:\n# Core Stack - template.yaml Outputs: ArticlesTableName: Description: Articles DynamoDB Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; ArticlesTableArn: Description: Articles DynamoDB Table ARN Value: !GetAtt ArticlesTable.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableArn\u0026#39; ImagesBucketName: Description: S3 Bucket for Images Value: !Ref ImagesBucket Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketName\u0026#39; UserPoolId: Description: Cognito User Pool ID Value: !Ref UserPool Export: Name: !Sub \u0026#39;${AWS::StackName}-UserPoolId\u0026#39; CloudFormation Imports Mechanism The importing stack (Service Stack) references exported values using !ImportValue:\n# Article Service Stack - template.yaml Parameters: CoreStackName: Type: String Description: Name of the core stack to import values from Default: travel-guide-core-staging Resources: CreateArticleFunction: Type: AWS::Serverless::Function Properties: Environment: Variables: ARTICLES_TABLE: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; IMAGES_BUCKET: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; USER_POOL_ID: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-UserPoolId\u0026#39; Policies: - DynamoDBCrudPolicy: TableName: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; - S3CrudPolicy: BucketName: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; Export Naming Conventions Best Practice: Include stack name in export name to avoid conflicts\n‚ùå Bad (can conflict across environments):\nExport: Name: ArticlesTableName # Conflicts if multiple environments ‚úÖ Good (unique per environment):\nExport: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; # Results in: travel-guide-core-staging-ArticlesTableName Naming Pattern:\n{StackName}-{ResourceType}{ResourceName} Examples: - travel-guide-core-staging-ArticlesTableName - travel-guide-core-staging-ArticlesTableArn - travel-guide-core-prod-ImagesBucketName Complete Example Core Stack Exports AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Core Infrastructure Resources: ArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#39;${AWS::StackName}-articles\u0026#39; BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: articleId AttributeType: S KeySchema: - AttributeName: articleId KeyType: HASH ImagesBucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#39;${AWS::StackName}-images\u0026#39; CorsConfiguration: CorsRules: - AllowedOrigins: [\u0026#39;*\u0026#39;] AllowedMethods: [GET, PUT, POST] AllowedHeaders: [\u0026#39;*\u0026#39;] Outputs: # Table Name Export ArticlesTableName: Description: Articles Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; # Table ARN Export ArticlesTableArn: Description: Articles Table ARN Value: !GetAtt ArticlesTable.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableArn\u0026#39; # Bucket Name Export ImagesBucketName: Description: Images Bucket Name Value: !Ref ImagesBucket Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketName\u0026#39; # Bucket ARN Export ImagesBucketArn: Description: Images Bucket ARN Value: !GetAtt ImagesBucket.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketArn\u0026#39; Service Stack Imports AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Article Service Parameters: CoreStackName: Type: String Description: Core stack name Default: travel-guide-core-staging Resources: # Lambda Function using imported values CreateArticleFunction: Type: AWS::Serverless::Function Properties: CodeUri: ./src Handler: create_article.handler Runtime: python3.11 Environment: Variables: # Import table name ARTICLES_TABLE: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; # Import bucket name IMAGES_BUCKET: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; Policies: # Grant DynamoDB access using imported ARN - Statement: - Effect: Allow Action: - dynamodb:PutItem - dynamodb:GetItem - dynamodb:UpdateItem Resource: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableArn\u0026#39; # Grant S3 access using imported ARN - Statement: - Effect: Allow Action: - s3:PutObject - s3:GetObject Resource: - !Sub - \u0026#39;${BucketArn}/*\u0026#39; - BucketArn: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketArn\u0026#39; Limitations and Workarounds Limitation 1: Cannot Delete Exporting Stack Problem: Cannot delete core stack while service stacks are importing its values.\nError:\nExport travel-guide-core-staging-ArticlesTableName cannot be deleted as it is in use by travel-guide-article-service-staging Workaround:\nDelete all importing stacks first Then delete the exporting stack # Delete service stacks first aws cloudformation delete-stack --stack-name travel-guide-article-service-staging aws cloudformation delete-stack --stack-name travel-guide-media-service-staging # Wait for deletion aws cloudformation wait stack-delete-complete --stack-name travel-guide-article-service-staging # Now delete core stack aws cloudformation delete-stack --stack-name travel-guide-core-staging Limitation 2: Cannot Change Export Name Problem: Cannot rename or delete an export while it\u0026rsquo;s being imported.\nWorkaround:\nCreate new export with new name Update all importing stacks to use new export Delete old export # Step 1: Add new export (keep old one) Outputs: ArticlesTableName: # Old export (keep for now) Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; ArticlesTableNameV2: # New export Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTable-Name\u0026#39; # Step 2: Update service stacks to use new export # Step 3: Remove old export from core stack Limitation 3: Cross-Region Not Supported Problem: Cannot import values from stacks in different regions.\nWorkaround: Use SSM Parameter Store for cross-region sharing:\n# Core Stack (us-east-1) Resources: TableNameParameter: Type: AWS::SSM::Parameter Properties: Name: /travelguide/core/articles-table-name Type: String Value: !Ref ArticlesTable # Service Stack (us-west-2) Parameters: ArticlesTableName: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/core/articles-table-name Best Practices Always include stack name in export names\nExport: Name: !Sub \u0026#39;${AWS::StackName}-ResourceName\u0026#39; Export both name and ARN for resources\nOutputs: TableName: Value: !Ref Table Export: Name: !Sub \u0026#39;${AWS::StackName}-TableName\u0026#39; TableArn: Value: !GetAtt Table.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-TableArn\u0026#39; Document exports in README\n## Core Stack Exports - `{StackName}-ArticlesTableName`: DynamoDB table name - `{StackName}-ArticlesTableArn`: DynamoDB table ARN - `{StackName}-ImagesBucketName`: S3 bucket name Use parameters for core stack name\nParameters: CoreStackName: Type: String Default: travel-guide-core-staging Plan exports carefully - they\u0026rsquo;re hard to change later\nViewing Exports AWS Console:\nCloudFormation ‚Üí Stacks ‚Üí Select Stack ‚Üí Outputs tab AWS CLI:\n# List all exports aws cloudformation list-exports # List exports from specific stack aws cloudformation describe-stacks \\ --stack-name travel-guide-core-staging \\ --query \u0026#39;Stacks[0].Outputs\u0026#39; Key Takeaways Exports allow sharing resources between stacks Imports reference exported values using !ImportValue Naming convention prevents conflicts: {StackName}-{ResourceName} Limitations exist: can\u0026rsquo;t delete exporting stack, can\u0026rsquo;t change export names Workarounds available for most limitations Documentation is crucial for team understanding "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/5.4.3-sqs-pipeline/","title":"SQS Pipeline","tags":[],"description":"","content":"Purpose Process images asynchronously to:\nReduce system load when multiple users upload simultaneously Ensure no message loss when Lambda encounters errors Separate processing steps (moderation ‚Üí detect labels) SQS Pipeline Architecture S3 Upload Event ‚Üì SQS: ModerationQueue ‚Üì Lambda: Content Moderation ‚Üì (if approved) SQS: DetectLabelsQueue ‚Üì Lambda: Detect Labels ‚Üì DynamoDB + Gallery Demo: SQS Queues Overview The system uses multiple SQS queues to process images in a pipeline:\nBenefits of SQS 1. Asynchronous Processing Lambda doesn\u0026rsquo;t need to wait for completion Users upload images quickly Processing happens in background 2. Automatic Retry If Lambda fails, message returns to queue Automatic retry with backoff Dead Letter Queue (DLQ) for repeatedly failed messages 3. Auto Scaling SQS automatically scales with message volume Lambda triggers in parallel for multiple messages No system overload concerns 4. Step Separation Content Moderation and Detect Labels are independent Easy to debug and monitor each step Simple to add new processing steps Queue Configuration ModerationQueue Visibility Timeout: 300s (5 minutes) Message Retention: 4 days DLQ: ModerationDLQ (after 3 retries) DetectLabelsQueue Visibility Timeout: 180s (3 minutes) Message Retention: 4 days DLQ: DetectLabelsDLQ (after 3 retries) Monitoring Track important metrics:\nApproximateNumberOfMessagesVisible - Messages waiting ApproximateAgeOfOldestMessage - Oldest message age NumberOfMessagesReceived - Total messages received NumberOfMessagesSent - Total messages sent Conclusion SQS Pipeline enables the system to:\nProcess reliably and stably Scale automatically based on demand Easy to maintain and extend "},{"uri":"https://tqunhh.github.io/FCJ-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.4-deployment-orchestration/","title":"Deployment Orchestration","tags":[],"description":"","content":"Deployment Orchestration with Bash Scripts Overview Deployment orchestration automates the process of deploying multiple CloudFormation stacks in the correct order with proper error handling and validation.\nDeployment Scripts Architecture scripts/ ‚îú‚îÄ‚îÄ deploy.sh # Main orchestration script ‚îú‚îÄ‚îÄ deploy-core.sh # Core stack deployment ‚îú‚îÄ‚îÄ deploy-service.sh # Service stack deployment template ‚îî‚îÄ‚îÄ utils.sh # Shared utility functions Main Orchestration Script deploy.sh - Orchestrates the entire deployment:\n#!/bin/bash set -e # Exit on error set -o pipefail # Catch errors in pipes # Configuration ENVIRONMENT=${1:-staging} REGION=${2:-us-east-1} CORE_STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; # Colors for output RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color # Error handling trap \u0026#39;echo -e \u0026#34;${RED}‚ùå Deployment failed${NC}\u0026#34;; exit 1\u0026#39; ERR echo -e \u0026#34;${GREEN}üöÄ Starting deployment to ${ENVIRONMENT}${NC}\u0026#34; # Step 1: Setup deployment bucket echo \u0026#34;üì¶ Setting up deployment bucket...\u0026#34; ./scripts/setup-bucket.sh $ENVIRONMENT $REGION # Step 2: Deploy Core Stack echo \u0026#34;üèóÔ∏è Deploying Core Stack...\u0026#34; ./scripts/deploy-core.sh $ENVIRONMENT $REGION # Wait for core stack to complete echo \u0026#34;‚è≥ Waiting for Core Stack...\u0026#34; aws cloudformation wait stack-create-complete \\ --stack-name $CORE_STACK_NAME \\ --region $REGION || \\ aws cloudformation wait stack-update-complete \\ --stack-name $CORE_STACK_NAME \\ --region $REGION # Verify core stack status CORE_STATUS=$(aws cloudformation describe-stacks \\ --stack-name $CORE_STACK_NAME \\ --region $REGION \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; \\ --output text) if [[ ! $CORE_STATUS =~ (CREATE_COMPLETE|UPDATE_COMPLETE) ]]; then echo -e \u0026#34;${RED}‚ùå Core stack deployment failed: $CORE_STATUS${NC}\u0026#34; exit 1 fi echo -e \u0026#34;${GREEN}‚úÖ Core Stack deployed successfully${NC}\u0026#34; # Step 3: Deploy Service Stacks SERVICES=(\u0026#34;auth\u0026#34; \u0026#34;articles\u0026#34; \u0026#34;media\u0026#34; \u0026#34;ai\u0026#34; \u0026#34;gallery\u0026#34; \u0026#34;notification\u0026#34;) for service in \u0026#34;${SERVICES[@]}\u0026#34;; do echo \u0026#34;üîß Deploying ${service} service...\u0026#34; ./scripts/deploy-service.sh $service $ENVIRONMENT $REGION # Optional: Wait for each service (sequential) # Or deploy all in parallel and wait at the end done echo -e \u0026#34;${GREEN}‚úÖ All services deployed successfully${NC}\u0026#34; echo -e \u0026#34;${YELLOW}üìä Deployment Summary:${NC}\u0026#34; echo \u0026#34; Environment: $ENVIRONMENT\u0026#34; echo \u0026#34; Region: $REGION\u0026#34; echo \u0026#34; Core Stack: $CORE_STACK_NAME\u0026#34; echo \u0026#34; Services: ${SERVICES[@]}\u0026#34; Core Stack Deployment Script deploy-core.sh:\n#!/bin/bash set -e ENVIRONMENT=$1 REGION=$2 STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; TEMPLATE_FILE=\u0026#34;infrastructure/core/template.yaml\u0026#34; PARAMS_FILE=\u0026#34;infrastructure/parameters/${ENVIRONMENT}.json\u0026#34; echo \u0026#34;Deploying Core Stack: $STACK_NAME\u0026#34; # Validate template echo \u0026#34;Validating template...\u0026#34; aws cloudformation validate-template \\ --template-body file://$TEMPLATE_FILE \\ --region $REGION # Convert parameters params_override=$(python3 -c \u0026#34;import json, sys; \\ data=json.load(open(\u0026#39;$PARAMS_FILE\u0026#39;)); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34;) # Package SAM template (if using SAM) echo \u0026#34;Packaging template...\u0026#34; sam package \\ --template-file $TEMPLATE_FILE \\ --s3-bucket travel-guide-deployment-${ENVIRONMENT} \\ --output-template-file /tmp/core-packaged.yaml \\ --region $REGION # Deploy stack echo \u0026#34;Deploying stack...\u0026#34; aws cloudformation deploy \\ --template-file /tmp/core-packaged.yaml \\ --stack-name $STACK_NAME \\ --parameter-overrides $params_override \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --region $REGION \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Core Stack deployed: $STACK_NAME\u0026#34; Service Stack Deployment Script deploy-service.sh:\n#!/bin/bash set -e SERVICE=$1 ENVIRONMENT=$2 REGION=$3 CORE_STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; STACK_NAME=\u0026#34;travel-guide-${SERVICE}-service-${ENVIRONMENT}\u0026#34; TEMPLATE_FILE=\u0026#34;infrastructure/services/${SERVICE}/template.yaml\u0026#34; echo \u0026#34;Deploying Service: $SERVICE\u0026#34; # Validate template aws cloudformation validate-template \\ --template-body file://$TEMPLATE_FILE \\ --region $REGION # Package template sam package \\ --template-file $TEMPLATE_FILE \\ --s3-bucket travel-guide-deployment-${ENVIRONMENT} \\ --output-template-file /tmp/${SERVICE}-packaged.yaml \\ --region $REGION # Deploy with core stack reference aws cloudformation deploy \\ --template-file /tmp/${SERVICE}-packaged.yaml \\ --stack-name $STACK_NAME \\ --parameter-overrides \\ CoreStackName=$CORE_STACK_NAME \\ Environment=$ENVIRONMENT \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --region $REGION \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Service deployed: $SERVICE\u0026#34; Deployment Flow Diagram flowchart TD Start[Start Deployment] --\u0026gt; Setup[Setup Deployment Bucket] Setup --\u0026gt; ValidateCore[Validate Core Template] ValidateCore --\u0026gt; PackageCore[Package Core Template] PackageCore --\u0026gt; DeployCore[Deploy Core Stack] DeployCore --\u0026gt; WaitCore[Wait for Core Complete] WaitCore --\u0026gt; CheckCore{Core Success?} CheckCore --\u0026gt;|No| ErrorCore[Show Error \u0026amp; Exit] CheckCore --\u0026gt;|Yes| DeployAuth[Deploy Auth Service] DeployAuth --\u0026gt; DeployArticle[Deploy Article Service] DeployArticle --\u0026gt; DeployMedia[Deploy Media Service] DeployMedia --\u0026gt; DeployAI[Deploy AI Service] DeployAI --\u0026gt; DeployGallery[Deploy Gallery Service] DeployGallery --\u0026gt; DeployNotif[Deploy Notification Service] DeployNotif --\u0026gt; Success[Deployment Complete ‚úÖ] ErrorCore --\u0026gt; End[End] Success --\u0026gt; End Error Handling Comprehensive error handling:\n#!/bin/bash # Exit on any error set -e # Exit on undefined variable set -u # Catch errors in pipes set -o pipefail # Cleanup function cleanup() { local exit_code=$? if [ $exit_code -ne 0 ]; then echo \u0026#34;‚ùå Deployment failed with exit code: $exit_code\u0026#34; echo \u0026#34;üìã Check CloudFormation events for details:\u0026#34; echo \u0026#34;aws cloudformation describe-stack-events --stack-name $STACK_NAME\u0026#34; fi } # Register cleanup on exit trap cleanup EXIT # Error handler for specific commands deploy_with_retry() { local max_attempts=3 local attempt=1 while [ $attempt -le $max_attempts ]; do echo \u0026#34;Attempt $attempt of $max_attempts...\u0026#34; if aws cloudformation deploy \u0026#34;$@\u0026#34;; then echo \u0026#34;‚úÖ Deployment successful\u0026#34; return 0 fi echo \u0026#34;‚ö†Ô∏è Attempt $attempt failed\u0026#34; attempt=$((attempt + 1)) sleep 10 done echo \u0026#34;‚ùå All attempts failed\u0026#34; return 1 } Rollback Strategies Automatic Rollback:\n# CloudFormation automatically rolls back on failure aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name my-stack \\ --disable-rollback false # Default behavior Manual Rollback:\n# Delete failed stack aws cloudformation delete-stack --stack-name my-stack # Redeploy previous version git checkout v1.2.3 ./deploy.sh staging Rollback to Previous Version:\n# List stack versions aws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE # Rollback using previous template aws cloudformation update-stack \\ --stack-name my-stack \\ --use-previous-template Validation Steps Pre-deployment validation:\n# 1. Validate template syntax aws cloudformation validate-template \\ --template-body file://template.yaml # 2. Lint template (using cfn-lint) cfn-lint template.yaml # 3. Check parameter file jq empty \u0026lt; parameters/staging.json # 4. Verify AWS credentials aws sts get-caller-identity # 5. Check stack dependencies aws cloudformation list-exports \\ --query \u0026#34;Exports[?Name==\u0026#39;travel-guide-core-staging-ArticlesTableName\u0026#39;]\u0026#34; Parallel Deployment Deploy services in parallel (faster):\n#!/bin/bash SERVICES=(\u0026#34;auth\u0026#34; \u0026#34;articles\u0026#34; \u0026#34;media\u0026#34; \u0026#34;ai\u0026#34; \u0026#34;gallery\u0026#34; \u0026#34;notification\u0026#34;) # Deploy all services in background for service in \u0026#34;${SERVICES[@]}\u0026#34;; do ./scripts/deploy-service.sh $service $ENVIRONMENT $REGION \u0026amp; done # Wait for all background jobs wait echo \u0026#34;‚úÖ All services deployed\u0026#34; Key Takeaways Orchestration scripts automate multi-stack deployment Error handling prevents partial deployments Validation catches issues before deployment Rollback strategies enable quick recovery Parallel deployment speeds up process Logging helps debug issues Best Practices Always validate templates before deployment Use set -e to exit on errors Implement retry logic for transient failures Log all operations for debugging Test scripts in staging before production Document deployment process in README "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/","title":"Image Processing - Lambda, Rekognition, SQS, SNS, SES","tags":[],"description":"","content":"Overview In the Travel Guide project, the automated image processing system includes the following key components:\nLambda Content Moderation - Content moderation for images Lambda Detect Labels - Automatic label detection SQS Queue - Asynchronous processing SNS Topic \u0026amp; SES - Email notifications Lambda Functions Architecture Processing Flow User uploads image ‚Üí S3 S3 Event ‚Üí SQS Queue Lambda Content Moderation checks content If valid ‚Üí Lambda Detect Labels adds tags If violated ‚Üí SNS + SES sends alert email Content Lambda Content Moderation Lambda Detect Labels SQS Pipeline SNS \u0026amp; SES Notification "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/5.4.4-sns-ses-notification/","title":"SNS &amp; SES Notification","tags":[],"description":"","content":"Purpose Send automatic email notifications when:\nContent policy violations are detected Admin needs to be notified about violations Users need to know their images were removed/quarantined SNS \u0026amp; SES Architecture Lambda Content Moderation ‚Üì (if violation detected) SNS Topic: ImageModerationAlerts ‚Üì ‚îú‚îÄ‚Üí SES: Email to Admin ‚îî‚îÄ‚Üí SES: Email to User Demo: SNS Topic Configuration SNS Topic is configured to distribute notifications to multiple subscribers:\nDemo: SES Verified Emails Before sending emails, addresses must be verified in SES:\nSNS Topic ImageModerationAlerts Topic Subscriptions:\nAdmin email (verified in SES) User email (verified in SES) Message Format:\n{ \u0026#34;articleId\u0026#34;: \u0026#34;article-123\u0026#34;, \u0026#34;imageKey\u0026#34;: \u0026#34;uploads/image.jpg\u0026#34;, \u0026#34;ownerId\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;violationType\u0026#34;: \u0026#34;explicit-content\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-08T10:30:00Z\u0026#34; } Email Templates Admin Notification Email Subject: ‚ö†Ô∏è Content Moderation Alert - Image Violation Detected Dear Admin, An image has been flagged for content policy violation: Article ID: {articleId} Image: {imageKey} Owner: {ownerId} Violation Type: {violationType} Severity: {severity} Action Taken: {action} Please review this case in the admin dashboard. --- Travel Guide Moderation System User Notification Email Subject: Your image was removed - Content Policy Violation Hello, We detected that one of your uploaded images violates our content policy: Article ID: {articleId} Violation Type: {violationType} Action: Image has been removed Please review our content guidelines and ensure future uploads comply with our policies. If you believe this was a mistake, please contact support. --- Travel Guide Team Demo Flow 1. Upload violating image User uploads inappropriate content S3 ‚Üí SQS ‚Üí Lambda Content Moderation 2. Detect violation Rekognition detect_moderation_labels returns violation Lambda determines severity level 3. Publish to SNS def send_admin_notification(article_id, key, moderation_result, owner_id): message = { \u0026#39;articleId\u0026#39;: article_id, \u0026#39;imageKey\u0026#39;: key, \u0026#39;ownerId\u0026#39;: owner_id, \u0026#39;violationType\u0026#39;: moderation_result[\u0026#39;labels\u0026#39;], \u0026#39;severity\u0026#39;: moderation_result[\u0026#39;maxSeverity\u0026#39;], \u0026#39;action\u0026#39;: \u0026#39;deleted\u0026#39;, \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat() } sns.publish( TopicArn=SNS_TOPIC_ARN, Subject=\u0026#39;‚ö†Ô∏è Content Moderation Alert\u0026#39;, Message=json.dumps(message) ) 4. SES sends emails Admin receives alert email User receives notification about removed image Demo Results ‚úÖ Admin Email:\nReceives violation notification Has complete information for review Can view details in dashboard ‚úÖ User Email:\nNotified about image removal Understands violation reason Knows how to contact support if needed Monitoring Track important metrics:\nSNS NumberOfMessagesPublished - Messages sent SES Send - Emails sent successfully SES Bounce - Bounced emails SES Complaint - Emails marked as spam Conclusion Automated notification system:\nEnsures admin is immediately notified of violations Users are transparently informed Reduces manual work Increases system professionalism "},{"uri":"https://tqunhh.github.io/FCJ-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in three events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 AM - 12:00 PM, Saturday, November 15, 2025\nLocation: AWS Office\nRole: Attendee\nDescription: Workshop providing comprehensive knowledge about AI/ML services on AWS, introducing Amazon SageMaker and Amazon Bedrock, hands-on chatbot building with GenAI, and exploring Prompt Engineering and RAG.\nEvent 2 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 8:30 AM - 5:00 PM, Monday, November 17, 2025\nLocation: AWS Office\nRole: Attendee\nDescription: Comprehensive workshop on DevOps practices on AWS, covering CI/CD pipelines, Infrastructure as Code (CloudFormation and CDK), container services (ECS/EKS), and monitoring \u0026amp; observability.\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 8:30 AM - 12:00 PM, Saturday, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Workshop covering 5 pillars of AWS Security: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response, with practical playbooks and best practices.\n"},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.4-image-processing/5.4.5-gallery-service/","title":"Gallery Service - Photo Discovery","tags":[],"description":"","content":"Gallery Service Overview Gallery Service provides photo discovery functionality through trending tags and tag-based search. It consumes data from AI Service\u0026rsquo;s label detection and presents it through user-friendly APIs.\nPurpose and Use Cases Main Functions:\nTrending Tags: Display most popular tags based on photo count Photo Gallery: Search photos by specific tags Position in System:\nConsumes data from AI Service (auto-detected tags) Provides API for Frontend to display gallery Reads from DynamoDB tables populated by AI Service Architecture Components Overview The Gallery Service consists of:\n2 Lambda Functions: GetTrendingTags, GetArticlesByTag 1 API Gateway: REST API with 2 endpoints 1 Lambda Layer: Shared dependencies (boto3, CORS utilities) 2 DynamoDB Tables: GalleryPhotosTable, GalleryTrendsTable (from Core Stack) Data Flow User Uploads Image ‚Üì Article Service ‚Üí S3 Bucket ‚Üì S3 Event ‚Üí AI Service ‚Üì Label Detection ‚Üì ‚îú‚îÄ‚Üí Save to GalleryPhotosTable ‚îî‚îÄ‚Üí Update GalleryTrendsTable (increment count) ‚Üì Gallery Service (Read-only) ‚Üì Frontend Display Dependencies with Core Stack Gallery Service imports from Core Stack:\n# From core-infra stack Imports: - GalleryPhotosTableName - GalleryTrendsTableName Deployment Order:\nCore Stack (creates tables) AI Service (populates tables) Gallery Service (reads tables) Lambda Functions Function 1: GetTrendingTagsFunction Purpose: Return list of trending tags sorted by popularity\nConfiguration:\nRuntime: Python 3.11 Timeout: 30 seconds Memory: 512 MB Handler: get_trending_tags.lambda_handler Logic Flow:\n1. Parse query parameters (limit) 2. Scan all tags from GalleryTrendsTable 3. Sort by count (descending) in-memory 4. Return top N tags Code Structure:\ndef lambda_handler(event, context): # Parse parameters limit = int(params.get(\u0026#34;limit\u0026#34;, 20)) # Scan all tags all_tags = [] while True: response = table.scan(**scan_kwargs) all_tags.extend(response.get(\u0026#39;Items\u0026#39;, [])) if \u0026#39;LastEvaluatedKey\u0026#39; not in response: break # Sort by count all_tags.sort(key=lambda x: x.get(\u0026#39;count\u0026#39;, 0), reverse=True) # Return top N return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;items\u0026#39;: all_tags[:limit], \u0026#39;total_tags\u0026#39;: len(all_tags) }) } Response Example:\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;tag_name\u0026#34;: \u0026#34;Beach\u0026#34;, \u0026#34;count\u0026#34;: 150, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; }, { \u0026#34;tag_name\u0026#34;: \u0026#34;Mountain\u0026#34;, \u0026#34;count\u0026#34;: 120, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/def456/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-14T15:20:00Z\u0026#34; } ], \u0026#34;total_tags\u0026#34;: 500 } Function 2: GetArticlesByTagFunction Purpose: Search photos with specific tag\nConfiguration:\nRuntime: Python 3.11 Timeout: 30 seconds Memory: 512 MB Handler: get_articles_by_tag.lambda_handler Logic Flow:\n1. Parse and validate tag parameter 2. Scan GalleryPhotosTable 3. Filter photos with matching tag (in-memory) 4. Limit results 5. Return photos Code Structure:\ndef lambda_handler(event, context): # Validate parameters tag = params.get(\u0026#34;tag\u0026#34;, \u0026#34;\u0026#34;).strip().lower() limit = int(params.get(\u0026#34;limit\u0026#34;, 50)) if not tag: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;tag parameter is required\u0026#39;}) } # Scan and filter matching_photos = [] while len(matching_photos) \u0026lt; limit: response = photos_table.scan(**scan_kwargs) for item in response.get(\u0026#39;Items\u0026#39;, []): photo_tags = [t.lower() for t in item.get(\u0026#39;tags\u0026#39;, [])] if tag in photo_tags: matching_photos.append(item) if \u0026#39;LastEvaluatedKey\u0026#39; not in response: break return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;items\u0026#39;: matching_photos[:limit]}) } Response Example:\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;photo_id\u0026#34;: \u0026#34;article-123-image-1\u0026#34;, \u0026#34;image_url\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;beach\u0026#34;, \u0026#34;sunset\u0026#34;, \u0026#34;ocean\u0026#34;], \u0026#34;status\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ] } Data Models GalleryPhotosTable Purpose: Store photo metadata and tags for gallery display\nSchema:\nField Type Description photo_id String (PK) Unique identifier (e.g., \u0026ldquo;article-123-image-1\u0026rdquo;) image_url String S3 key (e.g., \u0026ldquo;articles/abc123/image.jpg\u0026rdquo;) tags String[] Combined user tags + auto-detected tags autoTags String[] Deprecated (backward compatibility) status String \u0026ldquo;public\u0026rdquo; or \u0026ldquo;private\u0026rdquo; created_at String ISO 8601 timestamp createdAt String Alias for frontend Indexes:\nPrimary Key: photo_id No GSIs (Global Secondary Indexes) Data Population:\nPopulated by AI Service\u0026rsquo;s save_to_gallery Lambda Triggered when AI detects labels in images Combines user-provided tags with AI-detected tags GalleryTrendsTable Purpose: Track trending tags and popularity statistics\nSchema:\nField Type Description tag_name String (PK) Tag name (e.g., \u0026ldquo;beach\u0026rdquo;) count Number Number of photos with this tag cover_image String S3 key for cover image (optional) last_updated String ISO 8601 timestamp Indexes:\nPrimary Key: tag_name No GSIs Data Population:\nUpdated by AI Service when photos are added/removed Increment count when new photo with tag is added Decrement count when photo is deleted API Endpoints Endpoint 1: GET /gallery/trending Purpose: Get list of trending tags\nRequest:\nGET /gallery/trending?limit=20 HTTP/1.1 Host: {api-gateway-url} Query Parameters:\nParameter Type Required Default Description limit integer No 20 Maximum number of tags to return Success Response (200):\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;tag_name\u0026#34;: \u0026#34;Beach\u0026#34;, \u0026#34;count\u0026#34;: 150, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ], \u0026#34;total_tags\u0026#34;: 500 } Error Response (500):\n{ \u0026#34;error\u0026#34;: \u0026#34;Internal error: Gallery Trends table not configured\u0026#34; } Frontend Integration:\nasync function getTrendingTags(limit: number = 20) { const response = await fetch( `${API_BASE_URL}/gallery/trending?limit=${limit}` ); if (!response.ok) { throw new Error(\u0026#39;Failed to fetch trending tags\u0026#39;); } return await response.json(); } // Usage const data = await getTrendingTags(10); console.log(data.items); Endpoint 2: GET /gallery/articles Purpose: Search photos by tag\nRequest:\nGET /gallery/articles?tag=beach\u0026amp;limit=50 HTTP/1.1 Host: {api-gateway-url} Query Parameters:\nParameter Type Required Default Description tag string Yes - Tag name to search (case-insensitive) limit integer No 50 Maximum number of photos to return Success Response (200):\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;photo_id\u0026#34;: \u0026#34;article-123-image-1\u0026#34;, \u0026#34;image_url\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;beach\u0026#34;, \u0026#34;sunset\u0026#34;, \u0026#34;ocean\u0026#34;], \u0026#34;status\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ] } Error Responses:\n// 400 - Bad Request { \u0026#34;error\u0026#34;: \u0026#34;tag parameter is required\u0026#34; } // 500 - Internal Server Error { \u0026#34;error\u0026#34;: \u0026#34;Internal error: Gallery Photos table not configured\u0026#34; } Frontend Integration:\nasync function getPhotosByTag(tag: string, limit: number = 50) { const response = await fetch( `${API_BASE_URL}/gallery/articles?tag=${encodeURIComponent(tag)}\u0026amp;limit=${limit}` ); if (!response.ok) { const error = await response.json(); throw new Error(error.error || \u0026#39;Failed to fetch photos\u0026#39;); } return await response.json(); } // Usage const photos = await getPhotosByTag(\u0026#39;beach\u0026#39;, 20); console.log(photos.items); Frontend Gallery Display Trending Tags View:\nSearch Results View:\nPerformance Analysis Current Performance Metrics Lambda Configuration:\nFunction Timeout Memory Avg Execution Cold Start GetTrendingTags 30s 512 MB ~2-5s ~1-2s GetArticlesByTag 30s 512 MB ~3-8s ~1-2s DynamoDB Operations:\nBoth functions use Scan operations (full table scan) No indexes utilized In-memory filtering and sorting Performance Bottlenecks 1. Full Table Scans Issue: Scan entire table on every request\nImpact:\nSlow performance with large tables (\u0026gt;10,000 items) High DynamoDB read capacity consumption Increased costs Solution: Add Global Secondary Indexes (GSI)\n2. In-Memory Sorting (GetTrendingTags) Issue: Load all tags into memory, sort, then return top N\nImpact:\nUnnecessary memory usage Slow with large datasets Lambda timeout risk Solution: Use DynamoDB GSI with count as sort key\n# Recommended GSI GalleryTrendsTable: GlobalSecondaryIndexes: - IndexName: CountIndex KeySchema: - AttributeName: dummy_pk # All items have same value KeyType: HASH - AttributeName: count KeyType: RANGE Projection: ProjectionType: ALL 3. In-Memory Filtering (GetArticlesByTag) Issue: Scan all photos, filter by tag in Lambda\nImpact:\nVery inefficient Doesn\u0026rsquo;t scale High latency Solution: Use DynamoDB GSI or ElasticSearch\n# Recommended GSI GalleryPhotosTable: GlobalSecondaryIndexes: - IndexName: TagIndex KeySchema: - AttributeName: tag KeyType: HASH - AttributeName: created_at KeyType: RANGE Projection: ProjectionType: ALL Note: DynamoDB doesn\u0026rsquo;t support array attributes in GSI. Need to denormalize data or use ElasticSearch.\n4. No Caching Issue: Every request queries DynamoDB\nImpact:\nUnnecessary load Higher costs Slower response times Solution: Add caching layer\nOptions:\nElastiCache Redis: For frequently accessed data CloudFront: For API responses Lambda in-memory cache: For small datasets Example with ElastiCache:\nimport redis redis_client = redis.Redis(host=REDIS_HOST, port=6379) CACHE_TTL = 300 # 5 minutes def get_trending_tags_cached(limit): cache_key = f\u0026#34;trending_tags:{limit}\u0026#34; # Try cache first cached = redis_client.get(cache_key) if cached: return json.loads(cached) # Query DynamoDB result = get_trending_tags_from_db(limit) # Save to cache redis_client.setex(cache_key, CACHE_TTL, json.dumps(result)) return result 5. No Pagination Issue: Return all results (up to limit)\nImpact:\nLarge responses Slow load times Poor user experience Solution: Implement cursor-based pagination\ndef lambda_handler(event, context): params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) limit = int(params.get(\u0026#39;limit\u0026#39;, 20)) cursor = params.get(\u0026#39;cursor\u0026#39;) # LastEvaluatedKey from previous request scan_kwargs = {\u0026#39;Limit\u0026#39;: limit} if cursor: scan_kwargs[\u0026#39;ExclusiveStartKey\u0026#39;] = json.loads(base64.b64decode(cursor)) response = table.scan(**scan_kwargs) result = { \u0026#39;items\u0026#39;: response[\u0026#39;Items\u0026#39;], \u0026#39;cursor\u0026#39;: None } if \u0026#39;LastEvaluatedKey\u0026#39; in response: result[\u0026#39;cursor\u0026#39;] = base64.b64encode( json.dumps(response[\u0026#39;LastEvaluatedKey\u0026#39;]).encode() ).decode() return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Optimization Recommendations Short-term Improvements (Quick Wins) Add Caching\nImplement CloudFront caching for API responses TTL: 5 minutes for trending tags Reduces DynamoDB load by 90%+ Optimize Lambda Memory\nTest with 256 MB instead of 512 MB May reduce costs without impacting performance Add Pagination\nImplement cursor-based pagination Improve frontend UX with infinite scroll Error Handling\nAdd retry logic for DynamoDB errors Implement circuit breaker pattern Long-term Enhancements Add DynamoDB GSIs\nCountIndex for GalleryTrendsTable TagIndex for GalleryPhotosTable (requires denormalization) Implement ElasticSearch\nBetter for complex tag queries Full-text search capabilities Faceted search support Add ElastiCache\nRedis for frequently accessed data Reduce DynamoDB costs Improve response times Implement Event-Driven Updates\nUse DynamoDB Streams Update cache automatically when data changes Maintain cache consistency Cost Estimation Current Costs (Estimated) Assumptions:\n10,000 photos in gallery 500 unique tags 1,000 requests/day DynamoDB:\nRead Capacity: ~$0.25/day (full scans) Storage: ~$0.01/day Lambda:\nInvocations: ~$0.20/day Duration: ~$0.10/day API Gateway:\nRequests: ~$0.04/day Total: ~$0.60/day or ~$18/month\nOptimized Costs (With Caching) With CloudFront + ElastiCache:\nDynamoDB: ~$0.03/day (90% reduction) Lambda: ~$0.03/day (90% reduction) ElastiCache: ~$1.50/day (t3.micro) CloudFront: ~$0.02/day Total: ~$1.58/day or ~$47/month\nNote: Higher upfront cost but better performance and scalability.\nKey Takeaways Gallery Service provides photo discovery through trending tags and search Current implementation uses full table scans (inefficient) Performance bottlenecks include no caching, no indexes, in-memory operations Quick wins include adding caching and pagination Long-term should add GSIs or ElasticSearch for better scalability Cost optimization requires balancing performance vs infrastructure costs "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.5-auth-cognito-iam/","title":"Cognito &amp; IAM - Authentication &amp; Authorization","tags":[],"description":"","content":"Secure Authentication for Web/Mobile Applications This section covers the implementation of AWS Cognito for user authentication and IAM for authorization in the Travel Guide Application.\nOverview AWS Cognito provides user management and authentication services for web/mobile applications without building a login system from scratch. Combined with IAM roles and policies, it creates a secure, scalable authentication and authorization system.\nKey Components AWS Cognito:\nUser Pool for user management Sign-up / Sign-in / OTP email verification JWT Token management (id_token, access_token, refresh_token) Frontend integration (React) Backend integration (API Gateway + Cognito Authorizer) IAM (Identity \u0026amp; Access Management):\nRoles for Lambda, ECS, EC2 Policies for S3, DynamoDB, CloudWatch access Trust policies and resource-based permissions Least privilege principle Architecture The authentication flow:\nUser signs up/signs in via Cognito Hosted UI Cognito issues JWT tokens Frontend stores and manages tokens API Gateway validates tokens using Cognito Authorizer Lambda functions access AWS resources using IAM roles Content AWS Cognito Setup IAM Roles \u0026amp; Policies "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.5-parameter-management/","title":"Parameter Management","tags":[],"description":"","content":"Parameter Management for Multiple Environments Overview Parameters allow us to deploy the same infrastructure templates to different environments (staging, production) with different configurations.\nParameter Files Structure We organize parameters in JSON files per environment:\nparameters/ ‚îú‚îÄ‚îÄ staging.json ‚îî‚îÄ‚îÄ prod.json Example Parameter File staging.json:\n{ \u0026#34;Environment\u0026#34;: \u0026#34;staging\u0026#34;, \u0026#34;CorsOrigin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;AdminEmail\u0026#34;: \u0026#34;admin-staging@example.com\u0026#34;, \u0026#34;LogRetentionDays\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;EnableDebugLogs\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;MinCapacity\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;MaxCapacity\u0026#34;: \u0026#34;10\u0026#34; } prod.json:\n{ \u0026#34;Environment\u0026#34;: \u0026#34;prod\u0026#34;, \u0026#34;CorsOrigin\u0026#34;: \u0026#34;https://travelguide.com\u0026#34;, \u0026#34;AdminEmail\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;LogRetentionDays\u0026#34;: \u0026#34;30\u0026#34;, \u0026#34;EnableDebugLogs\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;MinCapacity\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;MaxCapacity\u0026#34;: \u0026#34;100\u0026#34; } Parameter Passing Mechanism Parameters are converted from JSON to CloudFormation format:\n# Convert JSON to AWS CLI format params_override=$(python -c \u0026#34;import json, sys; \\ data=json.load(sys.stdin); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34; \u0026lt; $PARAMS_FILE) # Deploy with parameters aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name my-stack \\ --parameter-overrides $params_override Template Parameter Definitions In CloudFormation template:\nParameters: Environment: Type: String Default: staging AllowedValues: [staging, prod] Description: Deployment environment CorsOrigin: Type: String Default: \u0026#34;*\u0026#34; Description: CORS origin for API Gateway AdminEmail: Type: String Description: Admin email for notifications LogRetentionDays: Type: Number Default: 7 Description: CloudWatch Logs retention in days EnableDebugLogs: Type: String Default: \u0026#34;false\u0026#34; AllowedValues: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;] Description: Enable debug logging Using Parameters in Resources Resources: CreateArticleFunction: Type: AWS::Serverless::Function Properties: Environment: Variables: ENVIRONMENT: !Ref Environment DEBUG: !Ref EnableDebugLogs CORS_ORIGIN: !Ref CorsOrigin ApiGateway: Type: AWS::Serverless::Api Properties: Cors: AllowOrigin: !Sub \u0026#34;\u0026#39;${CorsOrigin}\u0026#39;\u0026#34; AllowHeaders: \u0026#34;\u0026#39;*\u0026#39;\u0026#34; LogGroup: Type: AWS::Logs::LogGroup Properties: RetentionInDays: !Ref LogRetentionDays Environment Separation Strategy Parameter Staging Production Reason CorsOrigin * https://domain.com Security LogRetention 7 days 30 days Cost vs compliance DebugLogs true false Performance MinCapacity 1 2 Availability MaxCapacity 10 100 Scale Sensitive Data Handling Current Approach (Basic):\n{ \u0026#34;DatabasePassword\u0026#34;: \u0026#34;hardcoded-password\u0026#34; } Recommended Approach (Secure):\n1. AWS Systems Manager Parameter Store:\nParameters: DatabasePasswordSSM: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/staging/db-password Resources: Function: Type: AWS::Serverless::Function Properties: Environment: Variables: DB_PASSWORD: !Ref DatabasePasswordSSM 2. AWS Secrets Manager:\nResources: DatabaseSecret: Type: AWS::SecretsManager::Secret Properties: Name: !Sub \u0026#39;${AWS::StackName}-db-secret\u0026#39; GenerateSecretString: SecretStringTemplate: \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;}\u0026#39; GenerateStringKey: \u0026#34;password\u0026#34; PasswordLength: 32 Function: Type: AWS::Serverless::Function Properties: Environment: Variables: SECRET_ARN: !Ref DatabaseSecret Deployment Script with Parameters #!/bin/bash set -e ENVIRONMENT=$1 # staging or prod PARAMS_FILE=\u0026#34;parameters/${ENVIRONMENT}.json\u0026#34; if [ ! -f \u0026#34;$PARAMS_FILE\u0026#34; ]; then echo \u0026#34;Error: Parameter file not found: $PARAMS_FILE\u0026#34; exit 1 fi # Convert JSON to parameter overrides params_override=$(python -c \u0026#34;import json, sys; \\ data=json.load(sys.stdin); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34; \u0026lt; $PARAMS_FILE) # Deploy stack aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name \u0026#34;travel-guide-${ENVIRONMENT}\u0026#34; \\ --parameter-overrides $params_override \\ --capabilities CAPABILITY_IAM \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Deployed to ${ENVIRONMENT}\u0026#34; Best Practices Never commit secrets to Git\nUse .gitignore for sensitive parameter files Use Parameter Store or Secrets Manager Validate parameters before deployment\n# Validate JSON syntax jq empty \u0026lt; parameters/staging.json Document parameters in README\n## Parameters - `Environment`: Deployment environment (staging/prod) - `CorsOrigin`: CORS origin for API Gateway - `AdminEmail`: Email for admin notifications Use parameter constraints\nParameters: InstanceType: Type: String AllowedValues: [t3.micro, t3.small, t3.medium] Default: t3.micro Environment-specific naming\nResources: Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#39;travelguide-${Environment}-images\u0026#39; Key Takeaways Parameter files enable environment-specific configurations JSON format is easy to read and maintain Conversion script transforms JSON to CloudFormation format Sensitive data should use Parameter Store or Secrets Manager Validation prevents deployment errors Documentation helps team understand parameters Common Pitfalls ‚ùå Hardcoding values in templates\n# Bad Environment: Variables: API_URL: \u0026#34;https://api.staging.example.com\u0026#34; ‚úÖ Use parameters\n# Good Environment: Variables: API_URL: !Sub \u0026#34;https://api.${Environment}.example.com\u0026#34; ‚ùå Committing secrets to Git\n{ \u0026#34;DatabasePassword\u0026#34;: \u0026#34;super-secret-password\u0026#34; } ‚úÖ Use Secrets Manager\nDatabaseSecret: Type: AWS::SecretsManager::Secret Properties: GenerateSecretString: {} "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building Travel Guide Application with AWS Serverless Overview This workshop guides you through building a complete Travel Guide Application using AWS Serverless services. You\u0026rsquo;ll learn how to design, implement, and deploy a production-ready application with modern cloud architecture patterns.\nThe application allows users to share travel experiences, upload photos, and discover destinations through an AI-powered gallery system.\nKey Technologies:\nBackend: AWS Lambda, API Gateway, DynamoDB AI Processing: Amazon Rekognition, SQS, SNS/SES Authentication: Amazon Cognito, IAM Storage \u0026amp; CDN: S3, CloudFront Infrastructure: AWS SAM, CloudFormation Content Workshop Overview Infrastructure as Code \u0026amp; Multi-Stack Architecture Backend API \u0026amp; Articles Service AI Image Processing Pipeline Authentication with Cognito \u0026amp; IAM CloudFront CDN \u0026amp; Location Services Security Implementation "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.6-cloudfront-s3-location/","title":"CloudFront, S3 Upload &amp; Location Service","tags":[],"description":"","content":"CloudFront (Static + Dynamic), S3 Upload Web, AWS Location Service This section covers the implementation of Amazon CloudFront for content delivery, Amazon S3 for image uploads using pre-signed URLs, and AWS Location Service for geocoding and mapping in the Travel Guide Application.\nOverview The Travel Guide application uses three key AWS services to deliver a fast, secure, and feature-rich experience:\nAmazon CloudFront:\nCDN for static web content (React build) CDN for images (articles, thumbnails, avatars, covers) HTTPS redirect and compression Origin Access Identity (OAI) for secure S3 access Amazon S3 Upload Web:\nPre-signed URLs for secure, direct uploads No AWS credentials needed on client Time-limited upload permissions (15 minutes) CORS configuration for browser uploads AWS Location Service:\nPlace Index for geocoding/reverse geocoding Esri data source DynamoDB cache for cost optimization Nominatim fallback for reliability Architecture The system operates in three main flows:\nFlow A ‚Äî Static Web User accesses React web application CloudFront serves as CDN CloudFront fetches content from S3 StaticSiteBucket (private) via OAI Users always redirected to HTTPS with default index.html Flow B ‚Äî Image Upload \u0026amp; Display Frontend calls /upload-url API to get pre-signed URL Frontend uploads image directly to S3 ArticleImagesBucket When displaying articles/images, frontend uses CloudFront domain for fast loading CloudFront maps image paths to S3 origin Flow C ‚Äî Map/Geocoding When creating/updating articles with coordinates, backend uses AWS Location Service Place Index Reverse geocoding: lat/lng ‚Üí place name Forward geocoding: text ‚Üí lat/lng (for search) DynamoDB cache reduces Location Service costs Nominatim fallback for reliability Content S3 Pre-signed URL Upload CloudFront CDN Configuration AWS Location Service Integration "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.2-iac-multistack/5.2.6-lessons-learned/","title":"Lessons Learned &amp; Best Practices","tags":[],"description":"","content":"Lessons Learned \u0026amp; Best Practices What Worked Well ‚úÖ 1. Multi-Stack Pattern Success: Significantly reduced deployment time and risk.\nEvidence:\nService deployments: 2-3 minutes (vs 15-20 minutes monolithic) Bug fixes deployed without affecting other services Teams worked independently without conflicts Recommendation: Use multi-stack for any microservices architecture.\n2. Cross-Stack References Success: Type-safe dependencies between stacks.\nEvidence:\nCloudFormation validates imports at deployment time No hardcoded ARNs or names in code Easy to track dependencies Recommendation: Always use exports/imports over hardcoding.\n3. Bash Scripts Success: Simple but effective orchestration.\nEvidence:\nEasy to understand and modify No additional tools required Works in any CI/CD system Recommendation: Start with bash, migrate to sophisticated tools only if needed.\n4. SAM Simplification Success: Reduced boilerplate significantly.\nEvidence:\nLambda + API Gateway: 10 lines (vs 50+ lines pure CloudFormation) Automatic IAM role generation Built-in best practices Recommendation: Use SAM for all serverless applications.\n5. Environment Separation Success: Clean separation via parameters.\nEvidence:\nSame templates for staging and prod Easy to add new environments No code duplication Recommendation: Always use parameters for environment-specific config.\nChallenges Faced ‚ö†Ô∏è 1. Learning Curve Challenge: Initial learning curve for cross-stack references.\nImpact: First deployment took 2 days to get right.\nSolution:\nCreated documentation with examples Established naming conventions Built reusable templates Lesson: Invest time in documentation upfront.\n2. Debugging CloudFormation Errors Challenge: CloudFormation error messages can be cryptic.\nImpact: Spent hours debugging \u0026ldquo;Resource failed to stabilize\u0026rdquo;.\nSolution:\nCheck CloudWatch Logs immediately Use aws cloudformation describe-stack-events Enable detailed logging in Lambda Lesson: Always check CloudWatch Logs first.\n3. Stack Deletion Dependencies Challenge: Cannot delete core stack while services are running.\nImpact: Manual cleanup required in specific order.\nSolution:\n# Created cleanup script ./scripts/cleanup.sh staging # Deletes in correct order: # 1. Service stacks # 2. Core stack # 3. Deployment bucket Lesson: Plan deletion strategy from the start.\n4. Export Naming Challenge: Changed export name, broke all importing stacks.\nImpact: Had to update all service stacks simultaneously.\nSolution:\nEstablished naming convention early Documented all exports Added validation in deployment scripts Lesson: Plan export names carefully - they\u0026rsquo;re hard to change.\n5. Sequential Deployment Challenge: Sequential deployment slow for many services.\nImpact: Full deployment took 15+ minutes.\nSolution:\nImplemented parallel service deployment Reduced to 5 minutes total Lesson: Parallelize independent operations.\nBest Practices üéØ 1. Export Naming Convention # ‚úÖ Good: Include stack name Export: Name: !Sub \u0026#39;${AWS::StackName}-ResourceName\u0026#39; # ‚ùå Bad: Generic name Export: Name: ResourceName 2. Stack Separation ‚úÖ Core Stack: - DynamoDB Tables - S3 Buckets - Cognito User Pools - VPC/Networking ‚úÖ Service Stacks: - Lambda Functions - API Gateway - SQS Queues - SNS Topics 3. Template Validation # Always validate before deploy aws cloudformation validate-template \\ --template-body file://template.yaml # Use linting tools cfn-lint template.yaml 4. Error Handling # Use set -e in all scripts set -e set -o pipefail # Add cleanup trap trap cleanup ERR EXIT 5. Documentation ## Stack Exports - `{StackName}-ArticlesTableName`: DynamoDB table name - `{StackName}-ArticlesTableArn`: DynamoDB table ARN ## Deployment ```bash ./deploy.sh staging us-east-1 Rollback ./rollback.sh staging #### 6. Testing Strategy ```bash # Test in staging first ./deploy.sh staging # Verify functionality ./test.sh staging # Deploy to production ./deploy.sh prod 7. Rollback Plan # Always have rollback ready git tag v1.2.3 git push --tags # If issues: git checkout v1.2.2 ./deploy.sh prod Recommendations for Improvements üöÄ 1. CI/CD Integration Current: Manual deployment via scripts.\nImprovement: Automate with GitHub Actions/GitLab CI.\nExample:\n# .github/workflows/deploy.yml name: Deploy on: push: branches: [main] jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Deploy to staging run: ./deploy.sh staging Benefit: Automatic deployment on merge.\n2. Parallel Service Deployment Current: Sequential service deployment.\nImprovement: Deploy services in parallel.\nExample:\nfor service in \u0026#34;${SERVICES[@]}\u0026#34;; do ./deploy-service.sh $service $ENV \u0026amp; done wait Benefit: 3x faster deployment.\n3. Parameter Store for Secrets Current: Secrets in parameter files (not committed).\nImprovement: Use AWS Systems Manager Parameter Store.\nExample:\nParameters: DatabasePassword: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/staging/db-password Benefit: Centralized secret management.\n4. CloudWatch Dashboards Current: Manual monitoring via console.\nImprovement: Automated dashboards for stack health.\nExample:\nDashboard: Type: AWS::CloudWatch::Dashboard Properties: DashboardBody: !Sub | { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ] } } ] } Benefit: Proactive monitoring.\n5. CloudFormation Guard Current: Manual policy validation.\nImprovement: Automated policy enforcement.\nExample:\n# rules/security.guard AWS::S3::Bucket { Properties.PublicAccessBlockConfiguration exists Properties.BucketEncryption exists } Benefit: Prevent security misconfigurations.\n6. Auto-Generated Documentation Current: Manual documentation.\nImprovement: Generate docs from templates.\nExample:\n# Use cfn-diagram cfn-diagram template.yaml \u0026gt; architecture.png Benefit: Always up-to-date documentation.\nKey Takeaways üìù Multi-stack pattern is essential for microservices Cross-stack references provide type-safe dependencies Bash scripts are sufficient for orchestration SAM significantly reduces boilerplate Documentation is crucial for team success Testing in staging prevents production issues Rollback plan is mandatory CI/CD integration should be next step Parallel deployment speeds up process Security should be automated with Guard rules Comparison with Alternatives Aspect CloudFormation/SAM Terraform AWS CDK Pulumi Our Experience ‚úÖ Positive N/A N/A N/A Learning Curve Medium Medium High High AWS Integration Excellent Good Excellent Good State Management Automatic Manual Automatic Cloud Multi-Cloud No Yes No Yes Cost Free Free Free Free Recommendation ‚úÖ For AWS-only For multi-cloud For complex logic For full language power Final Thoughts The multi-stack pattern with CloudFormation/SAM proved to be the right choice for our Travel Guide Application. While there were challenges, the benefits far outweighed the costs:\nWins:\n‚úÖ Faster deployments (2-3 min vs 15-20 min) ‚úÖ Reduced blast radius ‚úÖ Independent team workflows ‚úÖ Easy rollbacks ‚úÖ Cost attribution per service Areas for Improvement:\nüîÑ CI/CD automation üîÑ Parallel deployments üîÑ Centralized secret management üîÑ Automated monitoring Would we do it again? Absolutely yes! ‚úÖ\n"},{"uri":"https://tqunhh.github.io/FCJ-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI directly participated in the Travel Journal project, which focuses on building a serverless web application on the AWS platform. Through this project, I was able to apply academic knowledge to practical scenarios while gaining exposure to teamwork and project implementation processes. As a result, my skills in programming, report writing, communication, and teamwork were significantly improved.\nIn terms of work attitude, I consistently made efforts to complete assigned tasks effectively, complied with company regulations, and actively communicated with colleagues to enhance overall work efficiency.\nHowever, during the internship, I recognized that I still need to further improve my discipline in time management as well as my problem-solving mindset, particularly when dealing with complex situations.\nOverall, the internship provided me with valuable hands-on experience, increased my confidence in engaging with real-world technology projects, and served as an important foundation for further developing both my technical skills and professional working attitude in the future.\nTo objectively reflect my internship experience, I hereby conduct a self-assessment based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Demonstrated solid understanding and continuous improvement in professional knowledge, especially in applying AWS services and web technologies to real-world projects. Tool proficiency and work quality improved noticeably throughout the internship period. ‚òê ‚úÖ ‚òê 2 Ability to learn Actively learned through documentation, guidance, and discussions with team members. Demonstrated a fairly good ability to quickly absorb new knowledge. ‚òê ‚úÖ ‚òê 3 Proactiveness Showed a strong sense of initiative at work, proactively researching and proposing solutions to technical issues. ‚úÖ ‚òê ‚òê 4 Sense of responsibility CDemonstrated a high level of responsibility, completing assigned tasks on time while meeting quality requirements of the project. ‚úÖ ‚òê ‚òê 5 Discipline Generally complied with common regulations; however, there is still room for improvement in time management and maintaining consistency in attendance and work progress. ‚òê ‚úÖ ‚òê 6 Progressive mindset Open to feedback from lecturers and team members, making adjustments and continuous improvements throughout the working process. ‚òê ‚úÖ ‚òê 7 Communication Able to clearly present ideas and report work progress. ‚úÖ ‚òê ‚òê 8 Teamwork Worked effectively within the team, actively cooperating and supporting other members, contributing to both project progress and team spirit. ‚úÖ ‚òê ‚òê 9 Professional conduct Maintained a serious and polite working attitude, showing respect toward colleagues and the working environment, demonstrating appropriate professional conduct. ‚úÖ ‚òê ‚òê 10 Problem-solving skills Able to identify problems and propose appropriate solutions. ‚òê ‚úÖ ‚òê 11 Contribution to project/team Actively contributed to the project by completing assigned tasks and supporting the team during the implementation phase. ‚úÖ ‚òê ‚òê 12 Overall Overall, successfully completed the internship, achieved the set objectives, and gained valuable practical experience beneficial for future studies and professional work.ion of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Enhancing discipline and time management: Further improvement is needed in strictly adhering to schedules and work plans, as well as proactively organizing tasks to ensure overall team progress. Strengthening problem-solving skills: Additional practice is required to analyze root causes more deeply and evaluate multiple solution options before selecting the most effective approach. Improving communication skills: Greater confidence is needed when presenting ideas, especially technical topics, and more proactive communication is encouraged when encountering difficulties to enhance team effectiveness. Further deepening technical expertise: More time should be devoted to studying advanced system architecture concepts and AWS services to improve solution quality in future project phases. "},{"uri":"https://tqunhh.github.io/FCJ-report/5-workshop/5.7-security/","title":"Security Implementation","tags":[],"description":"","content":"Security Implementation for Travel Guide Application This section covers the implementation of critical security improvements for the Travel Guide Application, focusing on data protection, input validation, and access control.\nOverview Security is paramount in any web application. The Travel Guide application handles user-generated content, personal data, and file uploads, making it essential to implement robust security measures.\nThree Critical Security Improvements:\nEncryption at Rest - Protect data stored in DynamoDB and S3 Input Sanitization - Prevent XSS and injection attacks S3 Ownership Validation - Prevent unauthorized file access Security Threats Addressed Threat Severity Impact Mitigation Data Breach üî¥ Critical Exposed user data Encryption at rest XSS Attacks üî¥ Critical Code injection HTML sanitization Unauthorized Access üü† High Data leakage Ownership validation File Abuse üü† High Storage costs Size/type validation Tag Spam üü° Medium Poor UX Tag limits Implementation Impact Before Security Updates:\n‚ùå Data stored unencrypted ‚ùå No input validation ‚ùå Users can access others\u0026rsquo; files ‚ùå No file size/type checks After Security Updates:\n‚úÖ All data encrypted (KMS/AES256) ‚úÖ HTML sanitization prevents XSS ‚úÖ Ownership validation enforced ‚úÖ File uploads validated Cost Impact Monthly Cost Increase: $5 (~20%) Breakdown: - DynamoDB KMS encryption: +$5/month - S3 AES256 encryption: FREE - Lambda execution: No change Total: $30/month (from $25/month) Worth it? ‚úÖ Absolutely! Security is not optional.\nContent Encryption at Rest Input Sanitization S3 Ownership Validation Key Takeaways Encryption at rest protects data from breaches Input sanitization prevents XSS and injection attacks Ownership validation prevents unauthorized access Security is continuous - regular audits needed Cost of security is minimal compared to breach costs "},{"uri":"https://tqunhh.github.io/FCJ-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe company provides a comfortable and professional working environment. Everyone is friendly, easy to communicate with, and always willing to support when problems arise. The workspace is tidy and quiet, which helps me stay focused and work more efficiently.\n2. Support from Mentor / Team Admin\nThe mentor played a crucial role in providing guidance throughout the internship. Whenever I encountered difficulties, I received clear and easy-to-understand explanations. The team admin also provided prompt support with administrative procedures, allowing me to focus fully on my technical tasks.\n3. Relevance of Work to Academic Major\nThe tasks assigned were closely aligned with my academic major. This allowed me to apply theoretical knowledge to real-world situations while also exposing me to new areas that were not deeply covered in my academic curriculum.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I developed not only technical skills but also important soft skills such as teamwork, time management, and professional communication. The practical experiences and insights shared by the mentor were highly valuable and helped shape my future career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company fosters a positive working culture that emphasizes collaboration and mutual respect. During high-pressure periods, team members consistently supported one another, making me feel respected and treated as a real member of the team rather than just an intern.\n6. Internship Policies / Benefits\nThe company provides flexible working hours, which are very suitable for students balancing academic commitments. In addition, interns are encouraged to participate in internal training sessions, enabling me to gain more practical knowledge and hands-on experience.\nAdditional Questions What did you find most satisfying during your internship?\n‚Üí What I appreciated most was the dedicated support from the mentor and the friendly working environment. The mentor not only provided guidance but also encouraged independent problem-solving, which helped me learn more from real-world scenarios. Moreover, the open culture and strong team spirit made me feel comfortable and not pressured as an intern. What do you think the company should improve for future interns?\n‚Üí I believe the company could provide more hands-on practice or assign smaller end-to-end tasks so interns can gain a more holistic view of the project lifecycle. Additionally, organizing more internal knowledge-sharing sessions or group mentoring activities could further enhance learning and experience exchange. If recommending to a friend, would you suggest they intern here? Why or why not?\n‚Üí I would definitely recommend this internship, especially to those who want to gain practical experience and develop real-world skills. The company offers a supportive environment, knowledgeable mentors, and a willingness to share expertise, ensuring that interns not only work but also learn valuable lessons for their future careers. "},{"uri":"https://tqunhh.github.io/FCJ-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tqunhh.github.io/FCJ-report/tags/","title":"Tags","tags":[],"description":"","content":""}]